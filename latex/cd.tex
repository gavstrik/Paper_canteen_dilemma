\def\arxiv{0}
\def\cogsci{1}

\if\arxiv 1
\documentclass[twocolumn,a4paper,superscriptaddress,nofootinbib]{revtex4}
\fi

\if\cogsci 1
\documentclass[a4paper]{article}
\usepackage[a4paper,top=2.5cm,bottom=2.5cm,left=2.5cm,right=2.5cm]{geometry}
\usepackage[natbibapa]{apacite}
\usepackage{times}
\fi

\newcommand{\tobo}[1]{{\color{red} TOBO: #1}}
\newcommand{\tsn}[1]{{\color{blue} TSN: #1}}
\newcommand{\re}[1]{{\color{brown} RE: #1}}


% Packages
\usepackage[caption=false]{subfig}  % needed for compatibility btw revtex and captions
\usepackage{dcolumn}   % needed for some tables
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
  \newtheorem{theorem}{Theorem}
  \newtheorem{lemma}[theorem]{Lemma}
  \newtheorem{definition}[theorem]{Definition}
\usepackage{tikz}
\usepackage[utf8]{inputenc} % for at få danske karakterer. Kræver at alle bruger en editor som understøtter utf8.
\usepackage{hyperref}

% Macros
\definecolor{mygreen}{rgb}{0,0.6,0}
\newcommand{\at}{\makeatletter @\makeatother}
\newcommand{\tmin}{t_{\min}}
\newcommand{\tmax}{t_{\max}}

% Style parameters
\if\arxiv 1 
\setlength{\parskip}{0pt}
\setlength{\tabcolsep}{6pt}
\setlength{\arraycolsep}{2pt}
\titlespacing*{\section}
{0pt}{2ex}{1ex}
\titlespacing*{\subsection}
{0pt}{2ex}{1ex}
\titlespacing*{\subsubsection}
{0pt}{2ex}{1ex}
\fi

\graphicspath{{../plots/}{../images/}}


\begin{document}
\if\cogsci 1
\setlength{\baselineskip}{1.44\baselineskip} % empirically this seems to match MS Word's idea of 1.5 line spacing % TSN: I found \linespread{1.25} to be Word's 1.5 line spacing, might be the same in effect.
% TOBO: 1.44\baselineskip is from the Danish National Research Councils and what they use to get the same effect in Latex as 1.5 line spacing in Word. Where did you find the suggestion to use \linespread{1.25}?
\fi


\title{The Curse of Shared Knowledge: Recursive Belief Reasoning in a Coordination Game with Imperfect Information}
\if\arxiv 1
\author{Thomas Bolander}
\affiliation{Department of Applied Mathematics and Computer Science, Technical University of Denmark, Richard Petersens Plads, building 324, DK-2800 Lyngby, Denmark}
\author{Robin Engelhardt}
\author{Thomas S. Nicolet}
\affiliation{Center for Information and Bubble Studies, Department of Communication, University of Copenhagen, Karen Blixens Plads 8, DK-2300 Copenhagen S.}
\fi

\if\cogsci 1
\author{Thomas Bolander, Robin Engelhardt, Thomas S. Nicolet}
\fi



%\significancestatement{Anybody who has been caught up in a sidewalk dance knows that human coordination without common knowledge is prone to failure. When common knowledge cannot be obtained, humans routinely act upon their private knowledge, or upon their shared reasoning about other minds. While we know much about human limits to such higher-order belief reasoning, little is known about at which point, and under which circumstances, shared knowledge becomes effectively indistinguishable from common knowledge. We investigate this question with a novel recursive coordination game with imperfect information, and show that people consistently act as if they have common knowledge about a fact, even in situations where they share only the shallowest levels of their state of mind.}

\maketitle

\begin{abstract}
Common knowledge is a necessary condition for safe group coordination. When common knowledge cannot be obtained, humans rely on their ability to identify shared knowledge to distinguish who knows what.
%When common knowledge can not be obtained, humans routinely use their ability to attribute beliefs and intentions \tobo{What does intentions have to do with it? Next sentence also says "*such* shared knowledge attributions", which in principle is neither about beliefs nor intentions.} \tsn {Maybe I misunderstand, but intentions are related due to the fact that coordination relies on being able to correctly attribute (predict) the intentions of others. But I find another issue: It might imply that people *only* attribute beliefs to others when common knowledge cannot be obtained. But every social action involves belief attribution (or attributing some mental state to others). So, could word it as something like "When common knowledge cannot be obtained, humans rely on their ability to identify shared knowledge to distinguish between who knows what" }  in order to infer what is known. 
But such shared knowledge attributions are limited in depth and therefore prone to coordination failures, because any finite-order knowledge attribution allows for an even higher order attribution that may change what is known by whom. In three separate experiments we investigate to which degree human participants (N=802) are able to recognize the difference between common knowledge and nth-order shared knowledge. We use a new two-person coordination game with imperfect information that is able to cast the recursive game structure and higher-order uncertainties into a simple, everyday-like setting. In the game, successful coordination to guarantee the highest possible payoff requires common knowledge among the participants. However, such common knowledge is impossible to achieve in the game, only nth-order shared knowledge can be achieved for varying values of n. Our results show that already from quite shallow depths of shared knowledge, the participants act as if they had common knowledge, and express the same certainty in their actions as if they had common knowledge. We call this phenomenon `the curse of shared knowledge'.
It occurs in spite of participants suffering huge payoff penalties when falsely assuming that coordination, and hence common knowledge, is guaranteed.
%express to be completely certain about being coordinated and achieving the highest payoff,  
% express to be as certain that they are coordinated and will achieve the highest payoff as if they indeed had common knowledge. 
%Despite this, our results show that participants behave as if they had common knowledge, and for sufficiently large values of n, they claim to  
% in self-assessing the certainty of receiving the 
%their actions
%claim their actions to be as certain as if they had common knowledge. 
%It is impossible to achieve common knowl
%
%Our results show that participants have a very hard time accepting the fact that common knowledge is not reducible to shared knowledge.\tobo{Maybe too strong? Do our results really show that? They behave as if having common knowledge, but they might still be able to distinguish common knowledge achieved via public announcements from what they have in this game.} Instead, participants try to coordinate even at quite shallow depths of shared knowledge and in spite of huge payoff penalties. \tsn{With regards to Tobo's comments consider the following alternative: "Our results show that participants try to coordinate when the levels of shared knowledge gets sufficiently high. In other words, our participants behave as if they have common knowledge about facts that are only shared knowledge, and collectively suffer huge payoff penalties as a result."} \re{Yes too strong, i guess. Proposal, taget fra TOBO: "Our results show that participants behave as if they had common knowledge. Participants also claim to be certain about their actions as if they had common knowledge, even when common knowledge is not obtainable."}
\end{abstract}


\section{Introduction}
Anybody who has been caught up in a sidewalk dance knows that human coordination without common knowledge about each others intentions is prone to failure. Thus, cooperating humans have evolved techniques by which to create common knowledge in practice, such as eye contact, conventions, and broadcasted messages. When common knowledge cannot be obtained, humans routinely act upon their private knowledge, or upon their shared reasoning about other minds. While we know much about human limits to such higher-order belief reasoning, little is known about at which point, and under which circumstances, shared knowledge becomes effectively indistinguishable from common knowledge. We investigate this question with a novel recursive coordination game with imperfect information, and show that people consistently act as if they have common knowledge about a fact, even in situations where they share only the shallowest levels of their state of mind. 

Successful group coordination requires complementary choices among group members, which, in turn, requires communication of beliefs and intentions in such a way that they become common knowledge~\citep{fagin1995reasoning}. A fact is said to be \textit{common knowledge} if everyone knows it, and everyone knows everyone knows it, and everyone knows everyone knows everyone knows it, and so on, ad infinitum~\citep{lewis1969convention, clark1981definite, schelling1980strategy, aumann1976agreeing}. If the premise of ``everyone knows'' is not infinitely nested, but only nested to finite depth, we instead have \emph{shared knowledge}. If there is no nested knowledge about knowledge at all and not everyone necessarily knows the fact, we have \emph{private knowledge}.

Let us illustrate the difference between these notions with an example. Two friends, Agnes and Bertram, are taking different trekking routes to the top of a mountain. In the morning they agree that if the weather gets bad, they will go back down and sleep in the mountain hut at the base. Otherwise, they strongly prefer to stay overnight at the top. The equipment essential for an overnight stay at the top has been divided between their backpacks. It is therefore crucial that if one of them decides to go to the top, the other one does the same. On the way to the top, they both observe a thunderstorm approaching, but are uncertain about whether the other person has seen it. At this point they both know the fact that ``a thunderstorm approaches'', but don't know whether the other knows. In this situation, we would say that Agnes and Bertram both have \emph{private knowledge} that a thunderstorm approaches, and since they both know it, it is also \emph{shared knowledge} between them. More generally, a fact is \emph{private knowledge} in a group of agents if some non-empty subset of the agents know the fact. For the special case where everybody in the group knows it, we say that there is \emph{shared knowledge to depth one} (or \emph{first-order shared knowledge}) of the fact~\citep{clark1981definite}.

Since Bertram doesn't know whether Agnes knows about the thunderstorm, he would like to warn her. So he sends a text message: ``Thunderstorm approaching. Let's meet at base.'' However, due to the unstable mobile network signals, he is not certain that the message will go through. Therefore he asks Agnes to confirm that she has received the  message. A few minutes later, he receives her confirmation. At this point it has become \emph{shared knowledge to depth two} (or \emph{second-order shared knowledge}) that a thunderstorm is approaching: She knows that he knows, since she received his message, and he knows that she knows, because he received her confirmation. To have \emph{shared knowledge to depth three} (\emph{third-order shared knowledge}), it would additionally be required that A) he knows that she knows that he knows, and B) that she knows that he knows that she knows. In fact, A already holds, since she confirmed receiving his message. However, B doesn't hold, since she will be uncertain about whether her confirmation was received in good order. Thus we have an asymmetry in the level of knowledge of the two agents. If she also asks him to confirm her message, and she receives such a confirmation, then of course she will get to know that he knows that she knows. Then there will be shared knowledge to depth three. However, there will still be a (higher-order) knowledge asymmetry, since Bertram can't be certain that the last message was received.

How many messages back and forth does it take for Agnes and Bertram to coordinate going back to the base in the evening? At first it might seem that it is sufficient for both of them to know that at least one of them plans to go to the base. However, that is not so. After the first message has been received, both know that Bertram plans to go to the base, but he is still uncertain whether she knows. And if she doesn't, he might risk leaving her alone at the top. So shared knowledge to depth one is clearly not sufficient. Even shared knowledge to depth two is not sufficient. After she has confirmed receiving his original message, from her perspective it is still entirely possible that he doesn't know that she knows, and hence, he might decide to go to the top to not leave her alone. And in this case, she also has to go to the top in order not to leave \emph{him} alone. This argument can be generalised to prove that even $n$th-order shared knowledge for any arbitrary large $n$ is insufficient for safe coordination.\footnote{To derive a contradiction, suppose $n$th-order shared knowledge for some $n$ is sufficient to make it safe to go to the base, in the sense of guaranteeing that the other person will also go there. This implies that the successful delivery of the $n$th message is sufficient to guarantee meeting at the base. There must then exist a smallest number $n_0$ such that the successful delivery of the $n_0$th message is sufficient to guarantee meeting at the base. Since $n_0$ is the smallest such number, the successful delivery of the $(n_0-1)$st message is not sufficient to guarantee meeting at the base. Now note that at the moment when the $n_0$th message has been successfully delivered, the sender of this message is still uncertain about whether it was actually received, and hence the sender is only certain that the first $n_0-1$ messages has been successfully delivered.  In other words, the sender of the $n_0$th message considers it possible---even after the successful delivery of the message---that only the first $n_0-1$ messages were successfully delivered, in which case it is \emph{not} safe to go to the base. Hence that person will after having sent the $n_0$th message stil consider it unsafe to go to the base, and will choose to go to the top. This is a contradiction, completing the proof. This proof is stated in rather informal terms, but can be turned into a formal, mathematical proof~\citep{fagin1995reasoning}.}  The consequence is that no finite number of messages successfully delivered will guarantee that Agnes and Bertram manage to meet at the base. In order to guarantee meeting at the base, they would need to have shared $n$th-order knowledge for \emph{all} $n \in \mathbb{N}$. Shared $n$th-order knowledge of some fact for all $n \in \mathbb{N}$ is called  \emph{common knowledge} of the fact. For a more formal definition of private, shared and common knowledge, see the Supplementary Material.\footnote{Supplementary Material is accessible at \url{http://www2.compute.dtu.dk/\~tobo/cd_sm.pdf}.}

In practice, shouldn't it be possible to coordinate meeting at the base after one or two messages being sent back and forth? Isn't it a purely mathematical problem with no practical implications for humans trying to coordinate their actions? One of the main goals of this paper is to argue that the answer to both questions is no. In order to make that argument, we have designed a coordination game that has a similar underlying mathematical structure as the example just given, but cast in a simpler, more everyday-like setting, where the higher-order uncertainty is established already when the game is initialised and not, as above, through a series of message passings. We originally developed the game to illustrate how the difference between shared and common knowledge can have a real impact on human behaviour. Humans generally find the concept of common knowledge hard to grasp, and even harder to grasp the practical relevance of, due to the unbounded nesting of knowledge.  The game we developed intends to make it clear that human intuitions about common knowledge can be misleading and may have costly consequences.

\section{The curse of shared knowledge}
Reasoning about the knowledge of others, their reasoning about you, and your reasoning about their reasoning, and so on, is famous in cognitive science for its presumed computational intractability~\citep{van2018parameterized}. Because of this,  coordinating species typically use heuristic shortcuts in order to work with nested knowledge states like common knowledge, such as joint perceptual cues and broadcasted signals~\citep{milgrom1981axiomatic, clark1996using, bradbury1998principles}. Humans may obtain common knowledge via mutually accessible first-order sensory experiences~\citep{tomasello1995joint, lorini2005establishing, bolander2015announcements, gintis2010rationality}, eye contact~\citep{friedell1969structure}, public rituals and conventions~\citep{lewis1969convention}, or salient focal points~\citep{schelling1957bargaining}. What most prominently is believed to distinguish human coordination from other animals, however, is the enormous flexibility by which humans can imagine and articulate the mental states of their peers~\citep{tooby2010groups, harari2014sapiens}. The abilities to blush, to tell jokes, and to write novels testify that humans readily attribute \textit{higher-order} beliefs, intentions, and reasoning capabilities to other people, such as thinking explicitly about the mental states of others who think about the thoughts and beliefs of others and so on---while at the same time appreciating that those thoughts and beliefs can differ from each other and from reality. Such higher-order cognition has seen substantial scientific attention, and has brought about various technical terms such as ``Theory of Mind'' (ToM)~\citep{premack1978does}, ``mentalizing''~\citep{frith2003development}, ``mind reading''~\citep{vogeley2001mind, apperly2010mindreaders}, ``mental models''~\citep{johnson1983mental}, ``mind perception''~\citep{gray2011distortions}, ``perspective taking'', and ``social intelligence''~\citep{baron1999social}, which often are used interchangeably for studying the cognitive mechanisms of shared knowledge, but sometimes focus on slightly different ideas and associated meanings~\citep{schaafsma2015deconstructing}, depending on the field of investigation.

Looking at the ToM-literature, conclusions about human belief reasoning abilities are rather heterogeneous~\citep{apperly2009humans, saxe2013theory}: Human reasoning about the reasoning processes of other humans is limited~\citep{premack1978does, gopnik1988children, stahl1995players, nagel1995unraveling, hedden2002you, keysar2003limits, pinker2003language}, contextual, and possibly domain specific~\citep{leslie1992domain, saxe2009theory, heyes2014submentalizing}. Three-year-old children tend to fail in the well-known first-order false belief tasks by falsely assuming that their private information is shared by others, while second-order false belief tasks are mastered around ages 5-7~\citep{perner1985john, sullivan1994preschoolers}. Adults may reliably master up to four orders~\citep{kinderman1998theory}, but still have difficulties ignoring the private information they possess when assessing the beliefs of others, resulting in a \textit{curse of knowledge} bias which can compromise their ability to make predictions about other people’s beliefs and actions~\citep{camerer1989curse, birch2007curse}. When the nested mental states represent a succession of different people, such as ``Alice thinks that Bob thinks that Carol is contemplating the idea that David is thinking about Evelyn'', we have less problems following along\footnote{Especially when those successions of mental states are qualified by psychological attribute words such as `Alice thinks that Bob is mistakenly worrying that Carol is offended by misunderstanding something Dave had said to Evelyn'.%~\citep{academian2019unrolling}.
} than when the nested mental states are successions of the same people over and over again, and thus are truly recursive, such as ``I think that you think that I contemplate the idea that you are thinking about me''. We get confused more easily by the latter formulation, since we need to keep track of several representations of ourselves and of the other, each representation differing in its perspective and in the number of mental states it presupposes~\citep{de2019common}. When humans compete or try to detect cheaters, higher-order belief reasoning seems to perform better than lower-order belief reasoning~\citep{goodie2012levels}. In negotiations and other mixed motive situations, where innuendo, threads, bribes and other kinds of indirect propositions are common, humans are very good at the \textit{strategic} use of higher-order belief reasoning, for instance as a means to \emph{prevent} common knowledge in certain groups of agents, or as a means to form specific knowledge alliances~\citep{pinker2007stuff, pinker2008logic, de2017negotiating}. ToM proficiency may also be facilitated by providing games with stepwise increase in ToM~\citep{verbrugge2018stepwise}.

In pure coordination problems, such as pedestrians choosing sides, or people agreeing on new words or on new technical standards, common knowledge is the preferred informational state for all members of the group, because it allows to coordinate on an optimal common equilibrium. If there are no or limited means by which to communicate, however, people face an equilibrium-selection problem for which neither game theory nor the ToM literature has any clear solution. Although some experimental evidence~\citep{curry2012putting} suggests that higher-order ToM reasoning may improve coordination efforts, other work seems to suggest that coordination favours lower orders of ToM sophistication~\citep{ devaine2014theory, de2015higher}. The challenge of tacit coordination is particularly relevant for artificial intelligence research and for social cognitive robotics, where the implementation of ToM-like processes into artificial social agents is believed to be an important step towards reliable human-robot interaction~\citep{erb2016artificial, bolander2018seeing, bard2020hanabi, dissing2020implementing}.

Recently, researchers have investigated whether humans have adapted specifically to recognizing common knowledge as a separate cognitive category, distinct from both private and shared knowledge~\citep{de2019common}. Controlled pure coordination experiments in social settings on market collaboration~\citep{thomas2014psychology}, the bystander effect~\citep{thomas2016recursive}, indirect speech~\citep{lee2010rationales}, self-conscious emotions~\citep{thomas2018common}, and charity~\citep{de2019maimonides}, consistently find that people indeed make strategically different choices under common knowledge conditions (presented in the form of public anouncements), compared to situations in which there is only private knowledge (in the form of private messages) or shared knowledge (private messages that elaborate on the depth of knowledge of other participants). Apart from seeing a clear benefit of common knowledge, some of these studies also showed that people have a hard time discriminating between various orders of shared knowledge, and that coordination efforts do not correlate with payoff conditions~\citep{thomas2014psychology}, which is in contrast to the assumptions of standard rational choice theory in which payoffs are expected to be maximized~\citep{becker1976economic}.

So humans are indeed able to discriminate common knowledge from shared knowledge, and adopt their actions accordingly, at least in cases where it is simple to discriminate the two states of knowledge. In the above-mentioned experiments, common knowledge was achieved via public announcements, and shared knowledge via private announcements in which the depth of knowledge was explicated. In other words, these were experiments in which the distinction between shared and common knowledge was made clear and explicit. The question then remains how good humans are at recognizing the difference between shared and common knowledge when the difference is not made explicit, but has to be deduced? The fact that humans are able to adopt their actions to whether they are in a shared or common knowledge situation might have significantly less practical relevance if humans in general have a hard time to distinguish these in the wild. The results of this paper suggest that humans are indeed not very good at recognizing the difference in the wild, in particular that they are prone to mistake $n$th-order shared knowledge for common knowledge---even for relatively low values of $n$. Or, at least, they behave as if they had common knowledge even when they only have $n$th-order shared knowledge for very moderate values of $n$. This issue has not been thoroughly investigated in previous research, in part 
%This is 
%and hence easily end up acting as if they had common knowledge in situations where they don't.   
% act differently in shared and common knowledge situations
%the difference between common knowledge and shared knowledge 
%So humans are indeed able to recognize common knowledge in situations where it is obviously achieved, like in the case of public announcements. They are also able to discriminate 
% \textit{when it is announced}. The question remains, however, how good humans are at recognizing the existence (or non-existence) of common knowledge \textit{when it is not announced, but deducible}. In other words, while humans are able to reliably infer common knowledge from broadcasted signals, how good are they at inferring common knowledge in situations that don't allow broadcasting of messages, but do allow for recursive belief reasoning about others? We do not know..."
%So if humans indeed have adapted to recognize common knowledge \textit{in the wild}, the question remains if they are also able to recognize the difference between common knowledge and $n$th-order shared knowledge for some (potentially large) $n$. \tobo{This is maybe a bit odd. The previous paragraph concludes that people *are* able to distinguish "common knowledge conditions" from other conditions, even shared knowledge ones. So I don't see why we are solving a "remaining question"? I wasn't aware of this earlier, as I hadn't looked so much at the papers cited above, but due to the review from PNAS, I started looking a bit into it. I think that also if we made two versions of the game, one with public announcements and the version we have now, people would probable notice the difference and be (even more) certain that they will successfully coordinate. However, it is hard to see how they could be even more certain than they already are in the violin plot (Figure 2. I'm not familiar with violin plots, but I don't quite get why there can be values above 'very certain' that ought to be max?). So maybe we are simply contradicting some of the earlier research? de2019common see a quite clear different between 3rd-order shared knowledge and common knowledge. But they also achieve 3rd-order in a quite different way: by *announcing* that it is 3rd-order shared knowledge. That probably makes it quite clear to the participants that they *don't* have common knowledge. In our game, the participants themselves have to reason about depths of knowledge, if they believe it relevant. So in a sense our game is more pure.} \tsn {Per Tobo's email: Maybe we  have to change the above and/or the below, to encapsulate the narrative that people act different under shared knowledge and common knowledge when the difference is made obvious (like with public announcements), but it is an open question if people act differently in 'the wild', where the difference is less obvious. In our game, the difference is not obvious, and participants do in fact act as if there was common knowledge. Indicates hypothesis that humans are accurate at detecting true common knowledge (we dont make false negatives), but are less accurate at detecting high levels of shared knowledge, since we report as common knowledge (false positives)} 
%\re{godt set TOBO. deFreitas et al.'s forsøgspersoner ser slet ikke common knowledge "in the wild". De hører det annonceret og agerer derefter. Og (måske) derfor kan de skelne mellem common- og shared knowledge. Enig med TSN at vi skal omformulere. Også enig i at "false positives" er det vores forsøg viser. Her et forsøg på en omformulering: "So humans are indeed able to recognize common knowledge \textit{when it is announced}. The question remains, however, how good humans are at recognizing the existence (or non-existence) of common knowledge \textit{when it is not announced, but deducible}. In other words, while humans are able to reliably infer common knowledge from broadcasted signals, how good are they at inferring common knowledge in situations that don't allow broadcasting of messages, but do allow for recursive belief reasoning about others? We do not know..." Ang. violinplot, så viser de de sandsynlige densiteter på en kontinuert skala. Vi kan tilføje en parameter cut=0 for at skære plottet af ved min og max, hvis I synes, men default er altså et smooth kontinuert plot.}
%In other words, while humans are able to reliably detect proper common knowledge in a wide range of situations, how good are they at refraining from inferring common knowledge in situations with only $n$th-order shared knowledge? We do not know, in part
 because many existing experimental designs stop after describing 2-3 orders of belief reasoning to the participants, as higher orders require quite convoluted sentences that tend to become incomprehensible and increase experimental error. Or, as in the mountain trekking example, they require reasoning about the consequences of a high number of (message passing) actions that each change the mental state of the involved agents.

The latter has been explored theoretically in the  `electronic mail game' by~\citet{rubinstein1989electronic}, a game version of the mountain trekking example presented above (and of the structurally equivalent `coordinated attack problem' by~\citet{fagin1995reasoning}). The Rubinstein paper shows that `almost common knowledge' in the sense of $n$th-order shared knowledge for some large $n$, leads to a very different expected player behaviour than `absolute common knowledge'. Essentially his conclusion, translated into the context of the mountain trekking example, is that common knowledge will make the two mountain hikers both go to the base, whereas if there is only $n$th-order shared knowledge for some $n$, then both will meet at the top, independent of $n$ and despite the bad weather condition (resulting in non-maximal payoffs). Rubinstein does a pure game-theoretic analysis of the game with no experiments, and only speculates what people playing the game might do. We find it interesting to dig deeper into how humans would play and reason about such games in practice. What would their intuition recommend them to do? Which depth of shared knowledge (if any) would be enough to attempt risky coordination that would lead to maximal payoff if successful? How would they be certain that the person they try to coordinate with thinks that the same depth is sufficient?

The electronic mail game and the mountain trekking example are complicated in terms of the dynamics of iterated message passing. In this paper, we devise a novel game in which the higher orders of shared knowledge are not achieved dynamically via actions, but are already present at the beginning of the game, using uncertainty about arrival times. This, we believe, makes the game easier to understand. Letting humans play our game, we have been able to address the previous questions in more detail. Our results indicate that the lack of common knowledge does not defer people from being confident in their ability to coordinate, except at the very lowest depths of shared knowledge. So people behave as if having common knowledge, despite only having a low level of shared knowledge, and despite the significant payoff penalties incurred.
%that the lack of common knowledge does not defer people from behaving as if they had common knowledge, resulting in significant payoff penalties.
%show that people indeed have a hard time accepting the fact that common knowledge is not reducible to shared knowledge of finite depth. \tobo{Again, the question is if this conclusion is too strong? Only our "do you have common knowledge" question actually indicates this. Also, of course, when I've talked to people, they claimed to have common knowledge, but we don't have data to back it up. We can say that our results "indicate" or something, but "show" is maybe too much. But what is true, of course, is that people behave *as if* they had common knowledge, and they even claim to be as certain about their actions *as if* they had common knowledge.} \tsn{Good point. We could angle it a bit towards behaviorism, and say that our results show that the lack of common knowledge does not seem to defer people from attempting to coordinate, except at 1-2 depths of shared knowledge. And then maybe that this indicates that people cannot distinguish between correctly between common knowledge and shared knowledge of certain depth. Consider "Our results show that people indeed behave as if they have common knowledge at sufficiently high levels of shared knowledge, resulting in significant payoff penalties.} \re{Enig. Jeg kan godt lide TSNs ændringsforslag: "Our results indicate that the lack of common knowledge does not defer people from attempting to coordinate, except at 1-2 depths of shared knowledge", og så fjerne den næste sætning der starter med "on the contrary..."} On the contrary, participants try to coordinate even at very low depths of shared knowledge and in spite of huge payoff risks. 
The reason, we believe, is that the sole presence of shared knowledge is enough to make participants try to coordinate, and that moderate depths of shared knowledge become effectively indistinguishable from common knowledge due to the recursive nature of the game. We call this effect ``the curse of shared knowledge'' because even small depths of shared knowledge raises the participant's expectation of being able to coordinate in spite of repeated payoff penalties for having miscoordinated before.

\section{Experimental Design}
The experiment is designed as a two-player coordination game with imperfect information. The game is inspired by the structure of the consecutive number riddle, also called the Conway paradox, see e.g.\ \citet{van1980conway,van2015one}. Our game is framed as an everyday situation, where two colleagues arrive at their workplace in the morning, and have to decide whether to meet in the canteen for a morning coffee or go straight to their offices and start working immediately. We call the game the `Canteen Dilemma'. The purpose of framing it in an everyday situation is to attempt to make some of the recursive reasoning easier to comprehend~\citep{meijering2010facilitative, wason1971natural}. The introductory story of the game goes as follows:
\begin{quote}
\indent
``Every morning you arrive at work between $8{:}10$ am and $9{:}10$ am. You and your colleague will arrive by bus 10 minutes apart. Example: You arrive at $\textbf{8:40 am}$. Your colleague may arrive at $\textbf{8:30  am}$, or $\textbf{8:50 am}$. Both of you like to meet in the canteen for a cup of coffee. If you arrive before 9:00 am, you have time to go to the canteen, but you should only go if your colleague goes to the canteen as well. If you or your colleague arrive at 9:00 am or after, you should go straight to your offices.''
\end{quote}
The game has 10 rounds on MTurk and participants are told that at the beginning of each round they will know only their own arrival time, and based on this will have to decide whether to go to the canteen or the office. After choosing an option, participants are asked to estimate their certainty that their colleague will choose the same option (on a five-point Likert scale). We call the value chosen the \emph{certainty estimate} of the participant. A fixed participation fee of \$2 is given to all players who finish the game. Additional bonuses are calculated with a logarithmic scoring rule whereby each participant is given an initial bonus of \$10, which is then reduced by a variable penalty in  each round, depending on the players' decisions and certainty estimates.

The game has three possible outcomes: 1) both choose the canteen which we refer to as \emph{coordination into the canteen}; 2) both choose their respective offices which we refer to as \emph{coordination into the offices}; 3) one chooses the canteen and the other chooses the office which we refer to as \emph{miscoordination}. Penalties are tiered in such a way that a small penalty is deducted for successful coordination into the canteen (achieving the highest payoff), which is doubled for coordination into the offices (achieving the second-highest payoff), while the penalty for miscoordination or forbidden choices, i.e.\ going to the canteen at 9 am or after, is much larger (up to 921 times larger, meaning a significantly lower payoff than the previous two). See Appendix~\ref{appendix:payoffs}, Payoffs and Penalties, % \tobo{Now Material and Methods is in the Appendix, so either the entire Appendix should be just that, or we should probably say "see the section on Material and Methods in the Appendix". Also CogSci wants section numbers, so we could also refer to it by number. Same for the later references to M and M.} \tsn {I assume you mean Material and Methods in the appendix in this document, and not the cd\_appendix, which does not contain it.} 
for details about the payoff structure. In the instructions shown to the participants beforehand, we also include payoff examples of both successful and failed coordinations. Screenshots and full descriptions of the experimental setup can be found in the Supplementary Material.

For the main experiment, we recruited a total of 680 participants from Amazon Mechanical Turk (MTurk) to play for a maximum of 10 rounds, making a total of $n=4260$ choices. In addition, we conducted two supplementary classroom experiments with 80 students (DTU1: $n=2160$) from the Technical University of Denmark (DTU) taking a course on Artificial Intelligence and Multi-Agent Systems, and 42 additional students (DTU2: $n=1012$) taking an introductory course in Artificial Intelligence. The two classroom experiments differed slightly from the MTurk experiment in that the students got an initial bonus (endowment) of \$30 instead of \$10, and played 30 rounds instead of 10. Also, in the classroom experiments, all students were told that they would not receive any monetary rewards for playing the game, but they should still try to do their best. The students also had to answer a few additional post-game questions, see the Supplementary Material for a full list of those questions.



\section{Game Strategies}
What are the relevant strategies for this game? First note that going to the canteen at $9{:}00$ or after results in the worst possible payoff. So both players should always go to the office if they arrive at $9{:}00$ or after. How about if both arrive strictly before $9{:}00$? If both choose canteen, they getter a better payoff than if both choose office. Now consider a case where you are one of the players, and you arrive at $8{:}50$. Then your colleague will be arriving at either $9{:}00$ or $8{:}40$. If your colleague arrives at $9{:}00$, she has to choose office according to the previous argument, and then you would have to choose office as well to avoid the large penalty of miscoordination. However, if your colleague arrives at $8{:}40$, you may both choose the canteen, and this will lead to the highest payoff. In other words, depending on the arrival time of your colleague, a piece of information that you don't have access to, the best choice is either office or canteen. So which one to choose?

Since the penalty of miscoordination is very high, it would seem best to choose office. What if you then instead  arrive at $8{:}40$? In this case, your colleague either arrives at $8{:}30$ or $8{:}50$. In both cases, you have time to meet for a cup of coffee in the canteen, and doing so will give you the highest payoff. At first, it might seem like an easy choice. However, we just concluded that the best strategy at $8{:}50$ would be to go to the office. So, if you arrive at $8{:}40$ and contemplate that your colleague might arrive at $8{:}50$---and if you believe your colleague would reason as yourself and go to the office at $8{:}50$---you also ought to go to the office at $8{:}40$. This argument can of course be iterated, because if the optimal choice at $8{:}40$ is to go to the office, then the optimal choice at $8{:}30$ must also be to go to the office. In other words, the optimal strategy seems to be to always go to the office, independent of arrival time! And, indeed, so it is. If both players go to the office in all rounds and declare the highest possible certainty in their decision, they will both leave the experiment with \$9.80, excluding the \$2 participation fee. This is the highest possible payoff that can be guaranteed by any strategy in the game, and very close to the \$10 that the players start out with. As we will see later, the payoffs that people actually get when playing the game are significantly lower than this.

The \emph{all-office} strategy described above, where you always decide to go to the office independent of arrival time, is a safe strategy if both players follow it. By safe is meant that there is never any risk of miscoordination, and hence no risk of getting the highest penalty (the penalty for miscoordination is up to \$9.21 in a single round). It is actually the \emph{only} safe strategy. The reason is that if at least one of the players, say $a$, has the strategy of going to the canteen at some time $t$ before $9{:}00$, then since they both have to go to the office at $9{:}00$ or later, there must exist at least one pair of arrival times for which the two players are miscoordinated.\footnote{Since $a$ chooses to go to the canteen at time $t$, player $b$ also has to go to the canteen at time $t+10$, since otherwise whey would be miscoordinated when $a$ arrives at $t$ and $b$ at $t+10$. But if $b$ goes to the canteen at time $t+10$, $a$ also has to go to the canteen at time $t+20$, since otherwise they would be miscoordinated when $b$ arrives at $t+10$ and $a$ at $t+20$. This can be generalized to conclude  that $a$ would have to go to the canteen at any time $t+20x$ for $x \geq 0$ and $b$ would have to go to the canteen at any time $t+10+20y$ for $y \geq 0$. Clearly this implies going to the canteen after $9{:}00$.}

The fact that the all-office strategy is the only safe one is counter-intuitive to most people before being presented with the proof, and for some people even after. The issue is that, intuitively, it would seem to be safe to go to the canteen at, say, $8{:}30$. Why would you ever go to the office that early? You know that your colleague will then be arriving at $8{:}40$, which is still plenty of time to get a cup of coffee before $9{:}00$. The issue is of course that if you take the perspective of your colleague, then your colleague arriving at $8{:}40$ will consider it possible that you arrived at $8{:}50$. And if you had indeed arrived at $8{:}50$, you would consider it possible that your colleague had arrived at $9{:}00$. In that case you would be forced to choose the office. A major point of our experiments is to test whether this kind of recursive perspective-taking is utilized by human players of the game.

The argument of the all-office strategy being safe of course relies on the other player following the same strategy. Since we don't allow players to agree on a strategy with their co-player beforehand, the all-office strategy doesn't necessarily in practice lead to the highest payoff for a particular player. Another issue is that one might decide to play risky instead of safe. Consider the \emph{canteen-before-9} strategy of always going to the canteen before $9{:}00$ and going to the office at later times, all with the highest certainty estimate. If both players choose this strategy and are fortunate to play 10 rounds without any of them arriving  at $9{:}00$ or later, they will get the highest possible payoff of \$9.90---slightly higher than the guaranteed payoff \$9.80 of the all-office strategy. However, if all pairs of arrival times are equally likely, the probability of miscoordination is then $1/6$ (there are 12 pairs of arrival times in total, and 2 of those have one player arriving at $8{:}50$ and the other at $9{:}00$). Miscoordination with the highest possible certainty estimate gives a penalty of \$9.21, so in practice this strategy is of course still significantly worse than the all-office strategy, even if only playing one round (the expected payoff for a single-round game will be $\$10.00 - \$9.21\cdot 1/6 - \$0.01\cdot 5/6 = \$8.46$).

In Appendix~\ref{appendix:formal}, Formal Analysis, 
%section \tobo{Is that the same as Material and Methods?} \tsn{Since Methods below include formal game strategies, it is correct here. We could change "Methods" to "Material and Methods" yes} 
we make the reasoning about game strategies formally precise. We show that independently of the particular payoff structure (only using the order of the payoffs, not their exact values), there will only be two candidates for the optimal strategy, the all-office strategy and the canteen-before-9 strategy. Which one is then optimal depends on the particular payoff structure and the number of arrival times before and after 9 am. In our specific experiments with our specific arrival times and payoff structure, the all-office strategy has a significantly higher expected utility than the canteen-before-9 strategy, as already argued. As we will see, the human players in our experiments very rarely play any of these strategies, but seem to believe that it is safe to go to the canteen if arriving sufficiently ahead of 9 am, e.g.\ before $8{:}50$, but unsafe when arriving later.

A strategy to always go to the canteen if arriving before some cut-off time $t_c$ and always go to the office if arriving after $t_c$ is called a \emph{cut-off strategy} (with cut-off $t_c$). The canteen-before-9 strategy is a cut-off strategy with cut-off $8{:}55$ (see Appendix~\ref{appendix:formal} for more details).

\section{Results}
\label{S:results}
\begin{table}%[h] 
\caption{} % N = number of subjects; R = maximum number of rounds; $\bar r$ = average number of rounds played; Ruin = percentage of participants loosing all their bonus before (or in) round R; Payoff = average earnings (given as the retained percentage of the initial endowment); $\bar s$ = average penalty per player per round.}

\smallskip
\centering
\setlength{\arrayrulewidth}{1pt}
\begin{tabular}{l|cccccr}
\hline
Experiment   		&   N$^*$ 	&   R$^*$ 	&  $\bar r^*$ & Ruin$^*$ (\%) 	& Payoff$^*$ (\%)	&  $\bar s^*$ (\$) \\
\hline
 MTurk      & 680 	& 10 	& 6.3 		 & 52.8 		& 23.6			& -1.59 \\
 DTU1       &  80 	& 30 	& 27.0		 & 17.5		& 27.0			& -0.83 \\
 DTU2       &  42 	& 30 	& 24.1		 & 31.0 		& 24.1			& -0.98 \\
\hline  \multicolumn{7}{c}{} \\[-2mm]
\multicolumn{7}{p{10cm}}{$^*$N = number of subjects; R = maximum number of rounds; $\bar r$ = average number of rounds played; Ruin = percentage of participants loosing all their bonus before (or in) round R; Payoff = average earnings (given as the retained percentage of the initial endowment); $\bar s$ = average penalty per player per round.} \\
\end{tabular}
\label{table:1}
\end{table}
The maximal theoretical payoff described in the previous section was never observed in the experiments---actually quite far from it, despite doing the experiment with more than $800$ people. Recall that the payoff of the all-office strategy is $\$9.80$ independent of arrival times. The average bonus paid to our MTurk participants was a mere $\$2.36$. Due to the penalty-based payoff structure, only 46 out of $340$ MTurk groups ($14\%$) were able to play $10$ rounds and still have any bonus left, while the average number of rounds played was $6.3$, see Table~\ref{table:1}. As soon as one of the players had no money left, the game would terminate.

Comparing the MTurk experiment with the DTU experiments in Table~\ref{table:1}, shows that the DTU participants were slightly better on average. While more than half of the MTurk participants had lost their initial bonus and had to end the game before the last round, only $18\%$ and $31\%$ of the DTU participants, respectively, had done so. Especially the students from the Artificial Intelligence and Multi-Agent Systems course (DTU1) managed well by retaining $27\%$ of the initial endowment and loosing only $\$0.83$ per round on average. This may come as no surprise, because these students were later into their studies, and had already been taught about social cognition.

\begin{figure} % [!b]
\centering\includegraphics[width=0.6\linewidth]{fig1_logit}
\caption{Frequency of canteen choices as a function of arrival times. Circles indicate the mean frequency of participants choosing the canteen at a certain arrival time with error bars. Colored lines are logistic regression lines with bootstrapped 95\% confidence intervals (10.000 resamples) shown as translucent bands. Fitted parameters show significant differences for all three experiments.}
\label{fig:1}
\end{figure}
Looking at Fig.~\ref{fig:1}, we see the frequency of participants' canteen choices as a function of their arrival time together with a fitted binary logistic regression line. While DTU students (orange and green) show similar steep profiles, MTurk participants have a slightly more gradual decline in canteen choices for increasing arrival times. However, the point at which there is a $50\%$ probability of choosing the canteen or the office is close to $8{:}50$ in all three experiments (see Appendix~\ref{appendix:logistic}). In Section~\ref{discussion}, Discussion, we therefore combine all three experiments (Fig.~\ref{fig:indistinguish}) in order to understand the experimental results in terms of degrees of shared knowledge.


\begin{figure} %[t]
\centering\includegraphics[width=0.7\linewidth]{fig2_certainties}
\caption{Violin plots of certainty estimates. In each round, participants were asked how certain they were of successful coordination with their colleague. Blue areas show the results from MTurk ($n=4260$) and orange areas show the results from DTU1 and DTU2 combined ($n=3172$). We predefined a five point likert scale of certainty estimates as: `very uncertain', `slightly certain, `somewhat certain', `quite certain', and `very certain', and translated them into the numerical values of probability estimates used in the payoff calculations (see Appendix~\ref{appendix:payoffs}).}
\label{fig:certain}
\end{figure}
Fig.~\ref{fig:certain} shows the distribution of certainty estimates for each arrival time. It clearly shows that it is exceedingly rare for any of the participants to consider it problematic to go to the canteen when arriving early. Arriving at $8{:}30$ or earlier is deemed sufficiently early to visit the canteen with very high confidence. %, and 
% \tobo{Isn't it more correct to say that that only happens at $8{:}30$ and that $8{:}40$, $8{:}50$ and $9{:}00$ all show some uncertainty? That's how it looks when I look at the violin plots. Only $8{:}30$ and earlier is really robust.} \re{agree. Someone please change to 8:30.} \tsn {Changed the text to 8:30, and adjusted text} 
%arriving at $9{:}00$ or later is deemed office time with high confidence, 
%arriving at $8{:}40$ is deemed either office or canteen with at least being ``somewhat certain''.
%and arriving at $8{:}50$ or $8{:}40$ is deemed either office or canteen with at least being ``somewhat certain''. 
The difference in certainty estimates between MTurk participants and DTU students show that the latter tend to be more certain that their co-players follow a similar strategy (higher certainty estimates for the early and late arrival times), and also that they are more aware of the danger of miscoordination (lower certainty estimates around the cut-off). This is in particular the case for the DTU1 experiment that has the steepest profile. Being more certain that your co-players follow a similar strategy probably indicates that you believe such a strategy to be optimal. So, interestingly, the DTU1 participants are both the ones that appear to be most aware of the danger of miscoordination, and at the same time those who most firmly believe a cut-off strategy is optimal, i.e., believing that the risk of miscoordination is unavoidable. The differences between the three experiments are however still relatively minor, and in the following we will combine data from all three experiments.


%\subsection{Group dynamics}
%\tobo{Does it still make sense to have this subsection called "group dynamics"? I also don't quite clearly see the relation between the content of the subsection and my normal understanding of the term "group dynamics" (and the Wikipedia entry on "group dynamics" for that matter).} \re{agree. Just start with "In figure xxx..."} \tsn {So, just delete the subsection header?}
\begin{figure} %[!b]
\centering\includegraphics[width=0.6\linewidth]{fig3_miscoordinations}
\caption{Number of coordinations and miscoordinations as a function of arrival times. Green means coordinating into the canteen, purple means coordinating into the office, and red means miscoordination. We use the notation 8:00/8:10 to denote the union of the arrival pairs $(8{:}00,8{:}10)$ and $(8{:}10,8{:}00)$, i.e., the arrival time combinations where one of the players arrive at $8{:}00$ and the other at $8{:}10$. Miscoordinations approach $50\%$ at 8:40/8:50 and 8:50/9:00.}
\label{fig:miscoordinations}
\end{figure}
%Starting with the group dynamics 
In Fig.~\ref{fig:miscoordinations}, we see the number of successful group coordinations into the canteen/office (green/purple) together with the number of miscoordinations (red) as a function of all possible arrival time combinations. The figure shows clearly that players are able to coordinate into the canteen more than 80\% of the time if both of them arrive before $8{:}50$. As soon as a group has a player who arrives at $8{:}50$, however, the result changes drastically. Suddenly almost half of such groups miscoordinate. As players experience harsh penalties for miscoordinating, one could perhaps expect to see a tendency of choosing office more often when arriving at 8:40 or 8:50 in subsequent rounds. That is, we might expect that players learn and converge to the all-office strategy in order to avoid miscoordination altogether. But this is not what we see.

\begin{figure} % [!t]
\centering\includegraphics[width=0.8\linewidth]{fig4_timeseries}
\caption{Mean frequencies of canteen choices for all possible arrival times as a function of the number of rounds played. The fitted straight lines are weighted linear squares (WLS) with the weights chosen to be the square root of the number of data points constituting the mean frequencies for each round, also shown by dot size.}
\label{timeseries}
\end{figure}
Fig.~\ref{timeseries} shows the mean frequency of canteen choices as a function of rounds played for all three experiments. Each color corresponds to a certain arrival time. Clearly, the only arrival times that do not converge towards either the canteen or the office are the arrival times of $8{:}40$ and $8{:}50$, with the former fluctuating around $90\%$ canteen choices and the latter fluctuating around $50\%$ canteen choices. This indicates that participants arriving at $8{:}40$ or $8{:}50$ do not feel incentivized to change their behavior significantly in subsequent rounds, even though there is a high risk of miscoordination. This is not to say that participants do not learn that canteen choices at $8{:}40$ or $8{:}50$ are dangerous. Partitioning the data from Fig.~\ref{fig:1} into two bins, corresponding to groups having had no miscoordination and groups having had one or more miscoordinations (see the supplementary data analysis in the Supplementary Material), shows somewhat decreasing certainty estimates around the critical arrival times, especially for DTU students. However, this does not affect their actual choices. MTurk participants do choose the canteen a little less often after a miscoordination (see Fig.~6 in the Supplementary Material), but this does not translate into better payoffs as later miscoordinations just move to earlier arrival times. So even though participants learn that their choices are risky, they don't see any way to improve their strategy. Specifically, they never converge to the optimal all-office strategy, and also not to the alternative canteen-before-9 strategy (cf.\ Theorem~\ref{theorem:all-office-or-cut-off} in Appendix~\ref{appendix:formal}). This apparent lack of behavioral change in higher-order social reasoning games is also shown by~\citet{verbrugge2008learning}. 

\section{Discussion}\label{discussion}
Let us try to analyse the experimental results in terms of the depth of knowledge of the participants. The highest payoff is achieved when successfully coordinating into the canteen before 9 am. With the aim of achieving the highest possible payoff, each participant can be expected to consider her own arrival time and try to assess whether there is still time to meet in the canteen. When a participant arrives strictly before 9 am, i.e.\ at $8{:}50$ or earlier, she has private knowledge that she arrives sufficiently early to go to the canteen. If participants only make choices based on their private knowledge, we should then expect participants to always go to the canteen at $8{:}50$. This is not what we see, cf.\ Fig.~\ref{fig:1}. Thus, other considerations in addition to the player's private knowledge must play a role in their decision-making.

\begin{figure} %[t]
\[
  \begin{tikzpicture}[align=left]
   \node[anchor=west] at (-2.6,-2.5) {players' knowledge level of\\ sufficient time for canteen:};
   \node[anchor=west,red] at (-2.6,1) {arrival time player 1:};
   \node[anchor=west,mygreen] at (-2.6,-1) {arrival time player 2:};
   \node[red] (n1820) at (3,1) {8{:}20};
   \node[mygreen] (n2820) at (3,-1) {8{:}20};
    \node[red] (n1830) at (5,1) {8{:}30};
   \node[mygreen] (n2830) at (5,-1) {8{:}30};
   \draw[-] (n1820) to (n2830);
   \draw[-] (n2820) to (n1830);
  \draw[dashed] (4,-3) to (4,1.5);
    \node[red] (n1840) at (7,1) {8{:}40};
   \node[mygreen] (n2840) at (7,-1) {8{:}40};
    \node[red] (n1850) at (9,1) {8{:}50};
   \node[mygreen] (n2850) at (9,-1) {8{:}50};
     \draw[-] (n2840) to (n1850);
   \draw[-] (n1840) to (n2850);
   \draw[-] (n1830) to (n2840);
   \draw[-] (n2830) to (n1840);
  \draw[dashed] (6,1.5) to (6,-3);
  \draw[dashed] (8,1.5) to (8,-3);
     \node[red] (n1900) at (11,1) {9{:}00};
   \node[mygreen] (n2900) at (11,-1) {9{:}00};
    \node[red] (n1910) at (13,1) {9{:}10};
   \node[mygreen] (n2910) at (13,-1) {9{:}10};
     \draw[-] (n2850) to (n1900);
   \draw[-] (n1850) to (n2900);
   \draw[-] (n1900) to (n2910);
   \draw[-] (n2900) to (n1910);
  \draw[dashed] (12,1.5) to (12,-3);
  \draw[dashed] (10,1.5) to (10,-3);
  \node at (13,-2.5) {none};
  \node at (11,-2.5) {none};
  \node at (9,-2.5) {private};
  \node at (7,-2.5) {shared\\ depth 1};
  \node at (5,-2.5) {shared\\ depth 2};
 \node at (3,-2.5) {shared\\ depth 3};
 \node at (7.8,0)
 	{\includegraphics[scale=.75]{fig5_logit_all}};
  \end{tikzpicture}
  \]
  \caption{The solid diagonal lines express indistinguishability for the players, e.g.\ the arrival time $8{:}40$ for player $1$ has a line to both of the arrival times $8{:}30$ and $8{:}50$ for player $2$, since these are the two arrival times for player $2$ that player $1$ will consider possible when herself arriving at $8{:}40$. Below each possible arrival time, we have marked the highest level of knowledge concerning whether there is sufficient time to go to the canteen, e.g.\ when arriving at $8{:}40$ there is shared knowledge to depth 1 of this fact, but not shared knowledge to depth $2$. In blue, a binary logistic regression model was used to predict the probability of a participant going to the canteen (upper limit) or to the office (lower limit) at the shown arrival times. The width of the regression line indicates the 95\% confidence interval using 10.000 bootstrapped resamples of all choices in all three experiments ($n=7432$).
  }\label{fig:indistinguish}
\end{figure}
When both participants know they arrived before 9 am, they have shared knowledge of having arrived in time for going to the canteen. This happens for any arrival pair $(t_1,t_2)$ with $t_i \leq 8{:}50$, $i=1,2$ (where, again, an arrival pair $(t_1,t_2)$ denotes that player $1$ arrives at time $t_1$ and player $2$ at time $t_2$). Note that for an arrival pair $(8{:}50,8{:}40)$, there is shared knowledge of there being sufficient time to go to the canteen, but only player $2$ knows this fact: Player $2$ knows that also player $1$ must have arrived before 9{:}00, but player $1$ doesn't know this about player $2$. In other words, when a player arrives at $8{:}50$, that player considers it possible that there is shared knowledge of being sufficient time for a cup of coffee in the canteen, but only if arriving at $8{:}40$ or before will that player \emph{know} there to be shared knowledge (to depth 1). When arriving at $8{:}30$ or before, the player additionally knows there to be shared knowledge to depth 2. We illustrate this in Fig.~\ref{fig:indistinguish}. Note that in general, if a player arrives at time $8{:}50-0{:}10n$, $n>0$, then that player knows that there is $n$th-order shared knowledge, but the player doesn't know there to be $(n+1)$st-order shared knowledge. This follows a similar pattern as the mountain trekking example, except here the depth of shared knowledge is determined by how early ahead of 9 am the agents arrive, rather than how many messages have successfully been delivered. No number of messages was sufficient to achieve common knowledge in the mountain trekking example. We similarly get that no arrival time is sufficiently early to establish common knowledge about having time to meet in the canteen. 

The participants seem to clearly be able to distinguish between private and shared knowledge, which is supported by their significantly different choices at $8{:}50$ and $8{:}40$ (see again Fig.~\ref{fig:indistinguish}). However, it is less clear whether they are able to robustly distinguish different levels of shared knowledge, and whether they are able to distinguish that from common knowledge. Indeed, most participants relatively robustly choose the canteen at $8{:}40$ and any time before that, despite the difference in depth of shared knowledge in those possible arrival times. The certainty estimates are however slowly decreasing from $8{:}10$ to $8{:}50$ in all three experiments (see Fig.~\ref{fig:certain}), showing that the participants are not completely ignorant to the differences. This could suggest that many participants are aware that it is less safe to go to the canteen based on $n$th-order shared knowledge than $(n+1)$st-order shared knowledge. However, very few seem to draw the conclusion that it is never safe to go to the canteen. Our game theoretic analysis showed that they ought to only choose the canteen when there is common knowledge that it is safe, which in this case actually means never.

Why do participants not regard earlier office choices as viable options? Why do participants not continue their train of thoughts and deduce that when $8{:}50$ turns out to be unsafe, $8{:}40$ will become unsafe as well, which means that $8{:}30$ will be unsafe also, etc.? One reason may be that the benefits of an all-office strategy are cognitively unavailable for the participants in the sense that participants have a limited ability to take the perspective of each other recursively. Another reason may be that the benefits of an all-office strategy are (vaguely) understood, but participants do not believe that their colleague will reason the same way as they do themselves, and instead try to guess what their colleague will choose. One candidate of such a (mixed) strategy may be the following: 1) always go to the canteen before $8{:}50$, 2) always go to the office after $8{:}50$, and 3) do some guesswork at $8{:}50$. The arrival time combinations $8{:}40/8{:}50$ and $8{:}50/9{:}00$ will then coordinate $50\%$ of the time, matching well with what we observe in Fig.~\ref{fig:miscoordinations}. If this is the strategy followed, players should also be aware of the $50\%$ probability of miscoordination at $8{:}50$. In Fig.~\ref{fig:certain}, we indeed see a much lower certainty estimate at those arrival times.

\begin{figure} %[h]
\centering\includegraphics[width=1\linewidth]{fig6_cutoff}
\caption{A) Frequencies of answers to the question: ``Imagine you could have agreed beforehand with your colleague about a point in time where it is safe to go to the canteen. What time would that be?'' Due to the pragmatics of language, we assume that an answer like 8:30 entails the belief that all earlier arrival times would also be deemed safe. B) Frequencies of answers to the question: ``Imagine you arrive at 8:10. Is it common knowledge between you and your colleague that it is safe to go to the canteen, that is, that you both arrived before 9:00?''}
\label{fig:5}
\end{figure}
Probing theses questions further, we asked participants the following post-game question:
\begin{quote}
\indent
``Imagine you could have agreed beforehand with your colleague about a point in time where it is safe to go to the canteen. What time would that be?'' \textit{('I don't know', 'There is no such time', 8:00, 8:10, 8:20, 8:30, 8:40, 8:50, 9:00, 9:10)}
\end{quote}
The results in Fig.~\ref{fig:5}A show that most answers range from 8:30 to 8:50 (approximately $75\%$ of all answers), giving support to the verdict that participants are not able to continue taking the perspective of each other recursively, or at least that they believe that shared knowledge of some modest finite depth is sufficient for the canteen choice to be safe. Rather, they stop after one or two, possibly three, iterations, thus believing that as long as they arrive sufficiently early, they can be sure to coordinate safely in the canteen. Notice that the correct answer ``there is no such time'' is chosen by less than $4\%$ of all participants, close to the margin of random error.


This indicates that participants indeed do believe that there exists a strategy that includes canteen choices without the risk of miscoordination, although in reality they would need common knowledge in order for the canteen choice to be without risk.
% supporting the conjecture stated in the introduction that moderate depths of shared knowledge become effectively indistinguishable from common knowledge.\tobo{Maybe this is one of the places where we could consider to moderate the claim. The question they are asked is not explicitly about common knowledge, and it is not about common knowledge as achieved via public announcements. The question doesn't in itself prove that they can't distinguish shared knowledge of moderate depth from common knowledge. But it shows that they will strategically behave as if they had common knowledge, and will not realise that the unsafety argument can be iterated. So they get "tricked" to believe they can safely coordinate, which means that in some sense their beliefs and actions compare to the common knowledge setting, but I'm not sure we can *strictly* claim the shared knowledge is indistinguishable from common knowledge?}  \tsn {Good point, and I'm not sure, difficult discussion. We want to say that our participants behavior is coextensive with common knowledge behavior. Not sure how to put it. It might be good for various reasons to not focus too much on mental states in general, and look at behavior, but this research field is generally not so behaviorist.} \re{agree as well. We can't really say anything about their understanding of common knowledge from this question. Proposal: remove text starting from ", supporting the conjecture".} \tsn {Support as well.} 
Participants might of course not necessarily have a precise idea of the technical notion of common knowledge and how it differs from $n$th-order shared knowledge, but as discussed in the introduction, there is actually quite a number of studies demonstrating that humans have adapted to recognize common knowledge and making distinct strategic choices depending on whether there is common, shared or private knowledge---at least in cases where the difference between these states of knowledge is relatively clear. In our experiments, we see the player behavior stabilizing already at relatively modest depths of shared knowledge, both in terms of action choices and certainty estimates. And that player behavior does not match what we would expect to see if they only believed to have $n$th-order shared knowledge, but instead matches what we would 
%matches what we would 
expect to see if they indeed wrongly inferred common knowledge. 

To specifically address the issue of whether they wrongly infer common knowledge, we asked a final post-game question:

\begin{quote}
\indent
``Imagine you arrive at $8{:}10$ am. Is it common knowledge between you and your colleague that it is safe to go to the canteen, that is, you both arrived before $9{:}00$ am?''. \textit{(`Yes', `No', `Don't know')}
\end{quote}

\noindent
This question inquires about participants' understanding of the term `common knowledge', and how it applies to the given situation. In Fig.~\ref{fig:5}B, the results show that $89\%$ of all participants responded that it was common knowledge that both players arrived before $9{:}00$ when they themselves had arrived at $8{:}10$. The answers may signify that indeed they believe there to be common knowledge in the strict technical (logical) sense. But of course the answers could also pertain to the everyday linguistic usage of the term ’common knowledge’, which is less strict.

\section{Conclusion} \label{S:conclusion}
We have devised a new coordination game, the Canteen Dilemma, to investigate human higher-order social reasoning. Our experimental results show that high levels of recursive perspective-taking are cognitively unavailable to the vast majority of players of the game. We see a significant amount of miscoordination, which seems to occur due to a ``curse of shared knowledge'': the guise of common knowledge existing in situations where there is only shared knowledge to some limited depth. 

Our experience from playing the Canteen Dilemma with many people and explaining to them its unintuitive result, is that many players simply do not accept the argument that there is no time early enough for them to coordinate safely into the canteen. On top of this, the certainty of participants that they will coordinate into the canteen at early arrival times indicates that when participants fail at $n$th-order reasoning, they do not default to agnosticism, but the opposite. That is, when there is a sufficiently large depth of shared knowledge about a fact, it is possible that such a fact is mistaken for proper common knowledge. An interesting avenue for future research would be to investigate if there may be any social and psychological benefits of having an {\em illusion of common knowledge}, such as a higher willingness to cooperate. Thus, what we have called the 'curse' of shared knowledge in the Canteen Dilemma, may turn out to be a blessing in other settings.

An obvious question is how often this illusion of common knowledge occurs in real life. For instance, in the real-world version of the Canteen Dilemma scenario, the two colleagues would be likely to simply coordinate their actions via cell phone (``I'll arrive at 8:50 today. Are you up for a cup of coffee in the canteen?''). This suggests that the advent of modern technology could have made the information asymmetry inherent in shared knowledge situations less widespread. However, modern technology has also given us the Internet and social media, where the flow of information is much more complex, creating more intricate cases of information asymmetry than ever before. 

That humans tend to confuse shared and common knowledge could possibly be due to a limited evolutionary importance of being able to make the distinction. It could also be due to the distinction requiring too many cognitive ressources. Or it could be that the confusion actually leads to evolutionary benefits in terms of higher degrees of cooperation in most practically occurring settings. What exactly has lead to the confusion, and to what degree it has any practical importance today, we leave as open problems.

\if\cogsci 1
\bibliographystyle{apacite} %model1-num-names}
\bibliography{cd}
\fi


\appendix

%\section{Materials and Methods}\label{sect:materials_methods}
% \tobo{Why is the section not called Material and Methods?} \tsn{As far as I can tell: We should change "Methods" here and above to "Material and Methods". We refer to both, and as far as I can see, the references are to material in the section below. UPDATE: Changed all instances of "Methods" to "Material and Methods"}

 %\small

\section{Experimental design and data collection} A large total sample size of $N=870$ was chosen to get robust conclusions from the statistical analysis while giving room for high variability in behavior. In fact, we thought that there would be a minority of groups who would meet in the office all the time. This turned out not to be the case. Experiments on Amazon Mechanical Turk had a total of 714 participants (including dropouts, see Supplementary Material), while the two classroom experiments at the Technical University of Denmark (DTU1 and DTU2) had a total of 106 and 50 participants, respectively\footnote{Complete anonymized data files and all code can be downloaded from \url{https://github.com/gavstrik/Paper_canteen_dilemma}.}

The average payout to MTurk workers was \$4.17 (including a general participation fee of \$2). After accepting our task and providing informed consent, participants from MTurk were put in a ’waiting room’ until they were paired up with another participant. After an instructions page, detailing the rules of the game, participants were given an arrival time $t \in \{8{:}00, 8{:}10, 8{:}20, 8{:}30, 8{:}40, 8{:}50, 9{:}00,9{:}10\}$ and asked to make a decision between between going to the canteen or to the office. Next, participants were asked to estimate how certain they were that their `colleague' made the same choice as them, ranging from `very uncertain' over `slightly certain', `somewhat certain' and `quite certain' to `very certain', which were translated  into numerical values, $e_i$, used in the payoff calculations (see below). A results page was shown between each round, showing the results of the previous rounds, including arrival times for both players, their choices, their own certainty estimates and resulting payoffs. After 30 seconds, the game would automatically proceed to the next round. After the last round, we asked all participants a few final questions about their strategy and their understanding of the game. The experiments were implemented using oTree 2.1.35~\citep{ChenSchongerWickens16}.

The two classroom experiments DTU1 and DTU2 differed from the MTurk experiment in a few aspects: 1) the maximum number of rounds played was increased from 10 to 30; 2) the initial bonus given each participant was increased accordingly from \$10 to \$30; 3) three additional questions were asked in order to elicit more explicitly some of the implicit assumptions and explicit behaviours by the students; 4) participants were told that they would not receive any monetary rewards, but that they should try to do their best. DTU1 received prizes. Screenshots, additional questions, experimental settings, and a detailed walk-through can be found in the Supplementary Material.

\subsection{Logistic regression} \label{appendix:logistic} 
The experimental results were analyzed using a logistic regression model with the arrival time $t$ as predictor. The model was specified as $\mu_i = \alpha _{i} + \beta _{i}t$, with $\mu_i$ being the log-odds $\mu_i= \log (p_i/(1-p_i))$ and $i=1,2,3$ denoting the MTurk, DTU1 and DTU2 experiments, respectively, allowing for differing slopes and intercepts. Results are shown in Fig.~\ref{fig:1}. The steepest slope occurs at $p(t) = 1/2 = -\alpha/\beta$, which for the MTurk and DTU1 experiments is $t=8{:}48$ and for DTU2 is $t=8{:}52$. The regression line in Fig.~\ref{fig:indistinguish} is obtained similarly by combining observations from all three experiments. The high number of observations imply small confidence bands. Hence, conclusions from the models can be viewed as robust.


\section{Payoffs and Penalties} \label{appendix:payoffs}
 All MTurk players finishing the game were paid a participation fee of \$2. In addition, a bonus could be earned if players did well. Before the game started, the bonus was set to \$10 for all participants. After each round, the bonus was reduced by a personal penalty, depending on the two players' choices. Penalties are calculated using a logarithmic scoring rule and by ordering them to be minimized by successful coordinations into the canteen. Penalties are maximized by any type of miscoordination or forbidden choice (i.e.\ going to the canteen at 9 am or later). Office coordinations are designed to have larger penalties than canteen coordinations, but smaller penalties than miscoordinations in order to make sure that coordination remains the main objective of the game. Penalties are defined as negative utility values in the following way. First, we define the chosen action $a_i$ by player $i$, $i=1,2$, to take binary values encoding the canteen option ($a_i=0$) and the office option ($a_i=1$), and define their respective certainty estimates $e_i \in \{0.5, 0.625, 0.75, 0.875, 0.99\}$. We can then express the utility $u$ received by player $1$ as $u(e_1, a_1, a_2) = (1-|a_1-a_2| + a_1a_2)ln(e_1) + 2|a_1-a_2|ln(1-e_1)$, and symmetric for player 2. If any of the players choose the canteen at 9 am or after, the utility becomes $u(e_1,a_1,a_2) = 2ln(1-e_1)$ for player 1 (and symmetric for player 2), corresponding to a miscoordination. As an example, imagine player 1 arrives at $8{:}40$ and choses the canteen, $a_1=0$. She estimates the probability that her colleague also will to go to the canteen to ``somewhat certain'', $e_1=0.75$. If her colleague indeed chooses the canteen, $a_2=0$, her utility will be $u(e_1,a_1,a_2) = ln(e_1) = -0.29$, but if her prediction proves false and her colleague chooses the office instead, her utility will be $u(e_1,a_1,a_2) = 2ln(1-e_1)=-2.77$. If she goes to the office just like her colleague, her utility is $u(e_1,a_1,a_2) = 2ln(e_1)=-0.58$. It should be noted that the logarithmic scoring rule used here is not strictly proper since office and canteen coordinations are penalized differently. Nevertheless, we find a good match between estimates and actual choices at arrival times different from those that are prone to miscoordinations, as seen in Fig.~\ref{fig:certain}, indicating that loss minimization remained a central concern and that participants made their choices and estimates as honestly as possible \citep{seidenfeld1985calibration,palfrey2009eliciting}.

\section{Formal Analysis} \label{appendix:formal}
The game can be represented as a game with three players, \emph{nature}, \emph{player 1} and \emph{player 2}. Nature is the player that initially decides the arrival times of player 1 and 2. Then player 1 and 2 are each informed of their own arrival time, and each have to choose among two actions: $o$ for going to the office and $c$ for going to the canteen. Based on the choice of actions by all three agents, player 1 and 2 receive a payoff, and they always receive the same payoff (we are disregarding the certainty estimates for now). The action choice of nature can be represented as an \emph{arrival pair} ${\bf t} = (t_1,t_2)$ consisting of the arrival time $t_1$ for player 1 and $t_2$ for player 2. Any arrival pair $\bf t$ has to satisfy that  $| t_1 - t_2 | = 10$ minutes (we will suppress mentioning the unit, minutes, in the following). In our specific version of the game, we additionally have the restriction that $8{:}10 \leq t_i \leq 9{:}10$ for $i = 1,2$. The analysis of optimal strategies however doesn't depend on the exact arrival times available, so we will make things a bit more general and only assume that there is an earliest arrival time $\tmin$ and a latest arrival time $\tmax$, and that $\tmin \leq 8{:}50$ and $\tmax \geq 9{:}00$. Given $\tmin$ and $\tmax$, the set of possible arrival pairs is defined as $T = \{ (t, t + 10) \mid \tmin \leq t \leq \tmax-10 \} \cup \{ (t, t-10) \mid \tmin+10 \leq t \leq \tmax \}$.

The game starts by nature choosing an element $\mathbf{t} \in T$. Nature is not a strategic player, so we assume that $\bf t$ is chosen uniformly at random, which is exactly how $\bf t$ is chosen in our experiments. The participants do not know that the arrival times are chosen uniformly at random, as this is left implicit in the description of the game. The following analysis of optimal strategies in the game could potentially change if arrival times were chosen according to a highly skewed probability distribution.

When nature has chosen its action ${\bf t} \in T$ and player 1 and 2 have chosen their actions $a_1$ and $a_2$, player 1 and 2 receive their payoff, which we denote $u_\mathbf{t}(a_1,a_2)$ (the utility resulting from player 1 choosing $a_1$ and player 2 choosing $a_2$ given that nature played $\bf t$). We don't need to make any assumptions regarding the exact utility values (payoff values), except that successful coordination into the canteen is always better than successful coordination into the offices, which again is always better than being miscoordinated. Hence, we put the following constraints on the utility function, for all  ${\bf t} \in T$,
\begin{enumerate}
   \item[(U1)] If $t_1,t_2 < 9{:}00$ then $u_\mathbf{t}(c, c) > u_\mathbf{t}(o, o) > u_\mathbf{t}(c,o) = u_\mathbf{t}(o,c)$.
   \item[(U2)] If $t_i \geq 9{:}00$ for some $i$, then $u_\mathbf{t}(o,o) > u_\mathbf{t}(c,o) = u_\mathbf{t}(o,c) = u_\mathbf{t}(c,c)$.
\end{enumerate}

A \emph{strategy} for player $i$, $i =1,2$, is a mapping from arrival pairs to actions, that is, a mapping $s_i: T \to \{c,o\}$. A strategy for $i$ simply determines which action $i$ will choose given the arrival pair. Each agent only observes her own arrival time, that is, any two arrival pairs $\mathbf{t}$ and $\mathbf{t}'$ with $t_i = t'_i$ will be indistinguishable to player $i$, $i=1,2$. This immediately leads to the following formal definition of the indistinguishability relation $\sim_i$ for player $i$: $\mathbf{t} \sim_i \mathbf{t}'$ iff $t_i = t'_i$. We need to require the strategy of each player to be \emph{uniform}, that is, any two arrival pairs that are indistinguishable by that player should be mapped to the same action: if $\mathbf{t} \sim_i \mathbf{t'}$ then $s_i(\mathbf{t}) = s_i(\mathbf{t'})$. Due to the uniformity condition, we can allow ourselves to overload the meaning of the symbol $s_i$ and write $s_i(t_i)$ as an abbreviation of $s_i(t_1,t_2)$ for $i=1,2$.

Note that the defined strategies are memoryless (Markov strategies), that is, a player's choice only depends on the observed arrival time in the current round, not the history of arrival times and chosen actions in earlier rounds. Human players playing the game should not be expected to necessarily play memoryless strategies, as they might seek to adopt to the observed strategy of the other player. However, since it is a repeated game (every round is a new instance of the same game), perfectly rational players playing the game for a sufficient number of rounds should converge to an optimal memoryless strategy. We will leave further discussion of history-dependent strategies and focus on the optimal memoryless strategies in the following.

Given strategies $s_1$ and $s_2$, the pair ${\bf s} = (s_1,s_2)$ is called a \emph{strategy profile}. Given an arrival pair ${\bf t} = (t_1,t_2)$, we use $\mathbf{s}(\mathbf{t})$ as a shorthand for $(s_1(t_1), s_2(t_2))$. Hence $\mathbf{s}(\mathbf{t})$ denotes the choices made by players 1 and 2 when their strategies are given by $\mathbf{s}$ and their arrival times are given by $\mathbf{t}$.
The payoff of those choices is then $u_\mathbf{t}(\mathbf{s}(\mathbf{t}))$. We will use $u_\mathbf{t}(\mathbf{s})$ as an abbreviation of $u_\mathbf{t}(\mathbf{s}(\mathbf{t}))$, i.e., $u_\mathbf{t}(\mathbf{s})$ is the utility received by player 1 and 2 when they play by strategy profile $\mathbf{s}$ in the game with arrival pair $\mathbf{t}$. The \emph{expected utility} $EU(\bf{s})$ of a strategy profile $\bf s$ is the average of the payoffs~\citep{shoham2008multiagent}:
\[
 EU({\bf s}) = \frac{1}{| T |} \sum_{{\bf t} \in T} u_\mathbf{t}(\mathbf{s}). %  \cdot P({\bf t}).
\]
Note again that player 1 and 2 get the same payoff (common-payoff game), so there is only one expected utility value to be computed. A strategy profile ${\bf s}'$ \emph{Pareto dominates} another strategy profile $\bf s$ if $EU({\bf s}') > EU({\bf s})$ \citep{shoham2008multiagent}.
A strategy profile is \emph{Pareto optimal} if there does not exist another strategy profile dominating it. A strategy profile ${\bf s}'$ \emph{weakly Pareto dominates} another strategy profile $\bf s$ if $EU({\bf s}') \geq EU({\bf s})$. The game is cooperative (between player 1 and 2), so both players should seek to play a Pareto optimal strategy profile. Also, since it is a common-payoff game, all Pareto optimal strategy profiles have the same expected utility~\citep{shoham2008multiagent}. 

Define subsets of arrival pairs $T_1, T_2 \subseteq T$ by:
\[
  \begin{array}{l}
  T_1 = \{ (8{:}10 + 20x, 8{:}20 + 20y) \in T \mid  y \leq x \leq y+1 \} \\
  T_2 = \{ (8{:}20 + 20y, 8{:}10 + 20x) \in T \mid  y \leq x \leq y+1 \}
  \end{array}
\]
Note that $T_1$ and $T_2$ are disjoint and that $T = T_1 \cup T_2$. We hence get, for any strategy profile $\bf s$,
\begin{equation}\label{equation:eu-split1}
 EU({\bf s}) = \frac{1}{| T |} \sum_{{\bf t} \in T} u_\mathbf{t}(\mathbf{s}) = \frac{1}{|T|} \sum_{i \in \{1,2\} } \sum_{{\bf t} \in T_i} u_\mathbf{t}(\mathbf{s})
\end{equation}
Given a strategy profile $\mathbf{s}$, we let $\mathbf{s} \upharpoonright T_i$ be the restriction of $\mathbf{s}$ to $T_i$, that is, $\mathbf{s} \upharpoonright T_i$ is as $\mathbf{s}$ except it is only defined on the arrival pairs in $T_i$. So $\mathbf{s} \upharpoonright T_i$ is the strategy profile for the subgame in which only the arrival pairs in $T_i$ can be chosen. We can now rewrite formula (\ref{equation:eu-split1}) as
\begin{equation}\label{equation:eu-split2}
 EU({\bf s}) = \frac{1}{| T |} \sum_{{\bf t} \in T} u_\mathbf{t}(\mathbf{s}) = \frac{1}{|T|}
\sum_{i \in \{1,2\}}
  \sum_{{\bf t} \in T_i} u_\mathbf{t}(\mathbf{s}  \upharpoonright T_i)
\end{equation}
Note that there exists no $\mathbf{t} \in T_1$, $\mathbf{t}' \in T_2$ and $i\in \{1,2\}$ such that $\mathbf{t} \sim_i \mathbf{t'}$. Hence the strategy profile $\mathbf{s} \upharpoonright T_1$ can be chosen completely independently of the strategy profile $\mathbf{s} \upharpoonright T_2$. Using formula (\ref{equation:eu-split2}) it then follows that a strategy profile $\mathbf{s}$ is Pareto optimal in the full game if and only if each of the strategy profiles $\mathbf{s} \upharpoonright T_1$ and $\mathbf{s} \upharpoonright T_2$ are Pareto optimal on the subgames with arrival pairs only in $T_1$ and $T_2$, respectively. When looking for Pareto optimal strategy profiles in the game, we can hence look for Pareto optimal strategy profiles on each of the two subgames independently (essentially the game consists of two disjoint subgames). Note also that the two games are completely symmetric, since $(t_1,t_2) \in T_1$ if and only if $(t_2,t_1) \in T_2$. Hence the two subgames necessarily have the exactly the same strategy profiles up to symmetry (swapping the roles of player 1 and 2). It is hence sufficient to only investigate Pareto optimal strategies for one of these subgames, say the subgame with arrival pairs in $T_1$.

We will now try to determine the possible candidates for being Pareto optimal strategy profiles for the subgame with arrival pairs in $T_1$. We do this by iteratively removing strategy profiles that are not Pareto optimal.
\begin{lemma}\label{lemma:no-canteen-at-nine}
Going to the canteen at $9{:}00$ or after can never be part of a Pareto optimal strategy profile. More precisely: No strategy profile $\bf s$ with $s_i(t_i) = c$ for some $i \in \{1,2\}$ and
$t_i \geq 9{:}00$  can be Pareto optimal.
\end{lemma}
\begin{proof}
Consider a strategy profile $\mathbf{s}$ with $s_{i}(t_{i}) = c$ for some $i\in \{1,2\}$ and
some ${\bf t} \in T_1$ with $t_{i} \geq 9{:}00$. We only consider the case of $i=1$, the case of $i=2$ being proved similarly. Then we have $s_1(t_1) =c$ and $t_1 = 9{:}10+20x$ for some $x$ (recall that we have restricted attention to the arrival pairs in $T_1$). Now define a strategy profile ${\bf s}'$ which is identical to ${\bf s}$ except $s'_j(t'_j) = o$ for all $j \in \{1,2\}$ and $t'_j \geq 9{:}00$. We want to show that ${\bf s}'$ Pareto dominates ${\bf s}$. First note that:
\[
  \begin{array}{rll}
   u_{(9{:}10+20x,9{:}00+20x)}({\bf s}') \!\!\!\!\!{ } &= u_{(9{:}10+20x,9{:}00+20x)}(o,o) &\text{def.\ of $\mathbf{s'}$} \\
   	&> u_{(9{:}10+20x,9{:}00+20x)}(c,\cdot) &\text{by U2} \\
	&= u_{(9{:}10+20x,9{:}00+20x)}({\bf s}), \\ 
  \end{array}
\]
where the last equality follows from $s_1(9{:}10+20x) = s_1(t_1) =  c$. This proves the existence of an arrival pair for which $\mathbf{s'}$ has a strictly higher utility than $\mathbf{s}$. To prove $EU(\mathbf{s'}) > EU(\mathbf{s})$, we hence only need to prove that $u_\mathbf{t'}(\mathbf{s'}) \geq u_\mathbf{t'}(\mathbf{s})$ for all $\mathbf{t'} \in T_1$. When $t'_1,t'_2 < 9{:}00$ this is trivial, as we then have $\mathbf{s'}(\mathbf{t'}) = \mathbf{s}(\mathbf{t'})$ by definition of $\mathbf{s'}$. When $t'_j \geq 9{:}00$ for some $j$, $\mathbf{s'}(\mathbf{t'})$ will by definition of $\mathbf{s'}$ necessarily have an $o$ in each position where $\mathbf{s}(\mathbf{t'})$ also has one. It follows by constraint U2 that $u_\mathbf{t'}( \mathbf{s'}) \geq u_\mathbf{t'}( \mathbf{s})$, as required.
\end{proof}
\begin{lemma}\label{lemma:no-c-after-o}
Assume $s_i(t) = o$ and $s_{3-i}(t+10) = c$ for some strategy profile $\bf s$, some arrival time $t$, and some $i \in \{1,2\}$. Then $\bf s$ is not Pareto optimal.
\end{lemma}
\begin{proof}
Let $\bf s$, $t$ and $i$ be as stated above. We need to find a strategy profile $\bf s'$ Pareto dominating $\bf s$. We only consider the case $i=1$, the case of $i=2$ being symmetric. Then we have $s_1(t)=o$ and $s_2(t+10)=c$.  If $s_j(t'_j) = c$ for some $j \in \{1,2\}$ and $t'_j \geq 9{:}00$, the existence of a strategy dominating $\bf s$ follows
immediately from Lemma~\ref{lemma:no-canteen-at-nine}. We can hence in the following assume that $s_j(t'_j) = o$ for all $j \in \{1,2\}$ and $t'_j \geq 9{:}00$. Since $s_2(t+10) =c$, we can thus also conclude that $t + 10 < 9{:}00$.

Now define $\bf s'$ to be identical to $\bf s$ except that we let
\[
  \begin{array}{ll}
    s'_1(t-20x) = c &\text{for } x \geq 0 \\
    s'_2(t-10-20y) = c &\text{for } y \geq 0,
  \end{array}
\]
recalling that we are only considering arrival pairs in $T_1$. We want to show that $EU(\mathbf{s'}) > EU(\mathbf{s})$. First note that
\[
 \begin{array}{rll}
 u_{(t,t+10)}({\bf s}')\!\!\!\!\!{ } &= u_{(t,t+10)} (c, s_2(t+10)) &\text{by def.\ of $\bf s'$} \\
 						&= u_{(t,t+10)} (c, c) &\text{by def.\ of $\bf s$} \\
						&> u_{(t,t+10)} (o,c) &\text{by U1, as } t+10 < 9{:}00 \\
						&= u_{(t,t+10)} ({\bf s}) &\text{by def.\ of $\bf s$}
  \end{array}
\]
To prove $EU(\mathbf{s'}) > EU(\mathbf{s})$, we hence only need to prove that $u_{\bf t'}({\bf s'}) \geq u_{\bf t'}({\bf s})$ for all $\mathbf{t'} \in T$. The only non-trivial cases are when either $t'_1 = t - 20x$ for some $x \geq 0$ or $t'_2 = t - 10 - 20y$ for some $y \geq 0$ (in all other cases, $\mathbf{s'}(\bf t') = \mathbf{s}(\bf t')$). Consider first a $\bf t'$ with $t'_2 = t -10- 20y$ for some $y \geq 0$. Then $t'_1 =  t - 20x$ for some $x \geq 0$, and hence $u_{\bf t'}( {\bf s'}) = u_{\bf t'} (c, c)$. Constraint U1 now immediately gives $u_{\bf t'} (c, c) \geq u_{\bf t'} (\bf s)$, and hence $u_{\bf t'}( {\bf s'}) \geq u_{\bf t'} (\bf s)$, as required. Consider instead $\bf t'$ with $t'_1 = t - 20x$ for some $x \geq 0$. Then either $t'_2 = t - 10 -20y$ for some $y \geq 0$ or we have $t'_1 = t$ and $t'_2 = t+10$. Both cases have already previously been covered. This completes the proof.
\end{proof}
\begin{definition}
A \emph{cut-off strategy with cut-off $t'$} is a strategy $s$ with $s(t) = c$ for all $t < t'$ and $s(t) = o$ for all $t > t'$. A \emph{cut-off strategy profile with cut-off $t'$} is a pair $(s_1,s_2)$ where both $s_1$ and $s_2$ are cut-off strategies with cut-off $t'$. A cut-off strategy (profile) with cut-off before $8{:}10$ is called an \emph{all-office strategy (profile)}.
\end{definition}
Note that the strategy we in the informal discussions above referred to as the ``canteen-before-9'' strategy is the cut-off strategy with cut-off $8{:}55$. We now get the result on optimal strategies claimed in the informal discussion.
\begin{theorem}\label{theorem:all-office-or-cut-off}
Any Pareto optimal strategy profile is either the all-office strategy profile or the cut-off strategy profile with cut-off $8{:}55$.
\end{theorem}
\begin{proof}
Let $\bf s$ be a Pareto optimal strategy profile. Assume that $\tmin$ is of the form $8{:}50-20x$ for some $x \geq 0$ and $\tmax$ is of the form $9{:}00+20y$ for some $y \geq 0$, the other cases being treated symmetrically. Let $\sigma$ be the following string over the alphabet $\{o,c\}$, where we alternate between the strategy choices of player 1 and 2 from $\tmin$ to $\tmax$:
\[
s_1(\tmin) s_2(\tmin + 10) s_1(\tmin + 20) s_2(\tmin + 30)  \cdots 
s_2(\tmax) \\
\]
Note that when $(t_1,t_2) \in T_1$, then $s_1(t_1)$ and $s_2(t_2)$ both occur in the string $\sigma$. From Lemma~\ref{lemma:no-c-after-o} it follows that $\sigma$ cannot contain the substring $oc$. Suppose the first letter of $\sigma$ is $o$. Then since $\sigma$ does not contain the substring $oc$, we have $\sigma = o^{|\sigma|}$ (a string of only $o$s). Hence $s_1(\mathbf{t}) = s_2(\mathbf{t}) = o$ for all $\mathbf{t} \in T_1$. This means that $\mathbf{s}$ is the all-office strategy profile. Suppose alternatively that the first letter of $\sigma$ is $c$. The last letter of $\sigma$ is necessarily $o$ since $\tmax \geq 9{:}00$ and $\bf s$ is Pareto optimal, cf.\ Lemma~\ref{lemma:no-canteen-at-nine}. Since $\sigma$ is then a string that starts with $c$ and ends with $o$, but doesn't contain $oc$, it must have the form $c^n o^m$ for some $n, m \geq 1$ with $m+n=|\sigma|$. Hence there exists a $t'$ such that $s_i(t) = c$ for all $t < t'$ and $s_i(t) = o$ for all $t > t'$ (and all $i \in \{1,2\}$).
In other words, $\mathbf{s}$ is a cut-off strategy profile with cut-off $t'$. What is left to prove is then only that $\mathbf{s}$ has cut-off $8{:}55$. First note that we must necessarily have $t' < 9{:}00$, since otherwise $\mathbf{s}$ would not be Pareto optimal according to Lemma~\ref{lemma:no-canteen-at-nine}. From this it follows that we for all arrival pairs $(t_1,t_2) \in T_1$ must have
\begin{enumerate}
  \item If $t_1,t_2 < t'$, the two players coordinate into the canteen, receiving the highest possible payoff.
  \item If $t_1 < t' < t_2$ or $t_2 < t' < t_1$, the two players are miscoordinated (one chooses canteen, the other office), receiving the lowest possible payoff.
  \item If $t_1,t_2 > t'$, the two players coordinate into their offices, receiving a payoff strictly between the lowest and highest.
\end{enumerate}
Note that there will always be exactly one arrival pair in $T_1$ of type 2, independent of $t'$. Since arrival pairs of type 1 have a higher payoff than arrival pairs of type 3, and $\mathbf{s}$ is Pareto optimal, $\mathbf{s}$ must have the maximal number of arrival pairs of type 1, that is, it is the cut-off strategy with the latest possible cut-off. That is exactly the cut-off $8{:}55$ (or, more precisely, any cut-off strictly between $8{:}50$ and $9{:}00$).
\end{proof}
The theorem proves what was argued in the main text: There are only two candidates for an optimal strategy, the all-office strategy or the canteen-before-9 strategy. This does not in any way imply that we should expect human players to adopt any of these two strategies, but if two perfectly rational players were to play the game, and if they knew they could expect the other player to play perfectly rational as well, of course the optimal strategy would be played. And, as earlier mentioned, in our particular version of the game, the optimal strategy is the all-office strategy.


%\subsection{Conflict of Interest}
%The authors declare no conflict of interest.

\if\arxiv 1
\begin{acknowledgments}
\else
\section{Author Contributions and Acknowledgments}
\fi
Thomas Bolander developed the game, made the theoretical analysis, and contributed as lead author; Robin Engelhardt designed the experiment, analyzed the data, and contributed as lead author; Thomas S. Nicolet designed the experiment, analyzed the data, and contributed as lead author. The authors declare no conflict of interest.

Server infrastructure and devops was handled by Mikkel Birkegaard Andersen. The authors wish to thank Vincent F. Hendricks for enabling the project. This research was approved by the Institutional Review Board at the University of Copenhagen and included informed consent by all participants in the study. The authors gratefully acknowledge the support provided by The Carlsberg Foundation under grant number CF 15-0212.

\if\arxiv 1
\end{acknowledgments}
\fi

%\subsection{Author contributions} % \small 

%\subsection{Open Research badges}
%\tobo{Fix this. Probably a footnote somewhere.}
%This article has earned Open Data badge. Data is available at www.github.com/gavstrik/cd.



\if\arxiv 1 
\begin{thebibliography}{71}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\providecommand{\bibinfo}[2]{#2}
\ifx\xfnm\relax \def\xfnm[#1]{\unskip,\space#1}\fi
%Type = Book
\bibitem[{Fagin et~al.(1995)Fagin, Halpern, Moses, and
  Vardi}]{fagin1995reasoning}
\bibinfo{author}{R.~Fagin}, \bibinfo{author}{J.~Y. Halpern},
  \bibinfo{author}{Y.~Moses}, \bibinfo{author}{M.~Y. Vardi},
  \bibinfo{title}{Reasoning About Knowledge}, \bibinfo{publisher}{MIT Press},
  \bibinfo{year}{1995}.
%Type = Book
\bibitem[{Lewis(1969)}]{lewis1969convention}
\bibinfo{author}{D.~Lewis}, \bibinfo{title}{Convention: A Philosophical Study},
  \bibinfo{year}{1969}.
%Type = Article
\bibitem[{Clark and Marshall(1981)}]{clark1981definite}
\bibinfo{author}{H.~H. Clark}, \bibinfo{author}{C.~R. Marshall},
\newblock \bibinfo{title}{Definite knowledge and mutual knowledge}
  (\bibinfo{year}{1981}).
%Type = Book
\bibitem[{Schelling(1980)}]{schelling1980strategy}
\bibinfo{author}{T.~C. Schelling}, \bibinfo{title}{The strategy of conflict},
  \bibinfo{publisher}{Harvard university press}, \bibinfo{year}{1980}.
%Type = Article
\bibitem[{Aumann(1976)}]{aumann1976agreeing}
\bibinfo{author}{R.~J. Aumann},
\newblock \bibinfo{title}{Agreeing to disagree},
\newblock \bibinfo{journal}{The annals of statistics}  (\bibinfo{year}{1976})
  \bibinfo{pages}{1236--1239}.
%Type = Article
\bibitem[{van~de Pol et~al.(2018)van~de Pol, van Rooij, and
  Szymanik}]{van2018parameterized}
\bibinfo{author}{I.~van~de Pol}, \bibinfo{author}{I.~van Rooij},
  \bibinfo{author}{J.~Szymanik},
\newblock \bibinfo{title}{Parameterized complexity of theory of mind reasoning
  in dynamic epistemic logic},
\newblock \bibinfo{journal}{Journal of Logic, Language and Information}
  \bibinfo{volume}{27} (\bibinfo{year}{2018}) \bibinfo{pages}{255--294}.
%Type = Article
\bibitem[{Milgrom(1981)}]{milgrom1981axiomatic}
\bibinfo{author}{P.~Milgrom},
\newblock \bibinfo{title}{An axiomatic characterization of common knowledge},
\newblock \bibinfo{journal}{Econometrica: Journal of the Econometric Society}
  (\bibinfo{year}{1981}) \bibinfo{pages}{219--222}.
%Type = Book
\bibitem[{Clark(1996)}]{clark1996using}
\bibinfo{author}{H.~H. Clark}, \bibinfo{title}{Using language},
  \bibinfo{publisher}{Cambridge university press}, \bibinfo{year}{1996}.
%Type = Article
\bibitem[{Bradbury et~al.(1998)Bradbury, Vehrencamp
  et~al.}]{bradbury1998principles}
\bibinfo{author}{J.~W. Bradbury}, \bibinfo{author}{S.~L. Vehrencamp}, et~al.,
\newblock \bibinfo{title}{Principles of animal communication}
  (\bibinfo{year}{1998}).
%Type = Article
\bibitem[{Tomasello(1995)}]{tomasello1995joint}
\bibinfo{author}{M.~Tomasello},
\newblock \bibinfo{title}{Joint attention as social cognition},
\newblock \bibinfo{journal}{Joint attention: Its origins and role in
  development}  (\bibinfo{year}{1995}) \bibinfo{pages}{103--130}.
%Type = Inproceedings
\bibitem[{Lorini et~al.(2005)Lorini, Tummolini, and
  Herzig}]{lorini2005establishing}
\bibinfo{author}{E.~Lorini}, \bibinfo{author}{L.~Tummolini},
  \bibinfo{author}{A.~Herzig},
\newblock \bibinfo{title}{Establishing mutual beliefs by joint attention:
  towards a formal model of public events},
\newblock in: \bibinfo{booktitle}{Proc. of CogSci}, pp.
  \bibinfo{pages}{1325--1330}.
%Type = Article
\bibitem[{Bolander et~al.(2015)Bolander, van Ditmarsch, Herzig, Lorini, Pardo,
  and Schwarzentruber}]{bolander2015announcements}
\bibinfo{author}{T.~Bolander}, \bibinfo{author}{H.~van Ditmarsch},
  \bibinfo{author}{A.~Herzig}, \bibinfo{author}{E.~Lorini},
  \bibinfo{author}{P.~Pardo}, \bibinfo{author}{F.~Schwarzentruber},
\newblock \bibinfo{title}{Announcements to attentive agents},
\newblock \bibinfo{journal}{Journal of Logic, Language and Information}
  (\bibinfo{year}{2015}) \bibinfo{pages}{1--35}.
%Type = Article
\bibitem[{Gintis(2010)}]{gintis2010rationality}
\bibinfo{author}{H.~Gintis},
\newblock \bibinfo{title}{Rationality and common knowledge},
\newblock \bibinfo{journal}{Rationality and Society} \bibinfo{volume}{22}
  (\bibinfo{year}{2010}) \bibinfo{pages}{259--282}.
%Type = Article
\bibitem[{Friedell(1969)}]{friedell1969structure}
\bibinfo{author}{M.~F. Friedell},
\newblock \bibinfo{title}{On the structure of shared awareness},
\newblock \bibinfo{journal}{Behavioral Science} \bibinfo{volume}{14}
  (\bibinfo{year}{1969}) \bibinfo{pages}{28--39}.
%Type = Article
\bibitem[{Schelling(1957)}]{schelling1957bargaining}
\bibinfo{author}{T.~C. Schelling},
\newblock \bibinfo{title}{Bargaining, communication, and limited war},
\newblock \bibinfo{journal}{Conflict Resolution} \bibinfo{volume}{1}
  (\bibinfo{year}{1957}) \bibinfo{pages}{19--36}.
%Type = Article
\bibitem[{Tooby and Cosmides(2010)}]{tooby2010groups}
\bibinfo{author}{J.~Tooby}, \bibinfo{author}{L.~Cosmides},
\newblock \bibinfo{title}{Groups in mind: The coalitional roots of war and
  morality},
\newblock \bibinfo{journal}{Human morality and sociality: Evolutionary and
  comparative perspectives}  (\bibinfo{year}{2010}) \bibinfo{pages}{91--234}.
%Type = Book
\bibitem[{Harari(2014)}]{harari2014sapiens}
\bibinfo{author}{Y.~N. Harari}, \bibinfo{title}{Sapiens: A brief history of
  humankind}, \bibinfo{publisher}{Random House}, \bibinfo{year}{2014}.
%Type = Article
\bibitem[{Premack and Woodruff(1978)}]{premack1978does}
\bibinfo{author}{D.~Premack}, \bibinfo{author}{G.~Woodruff},
\newblock \bibinfo{title}{Does the chimpanzee have a theory of mind?},
\newblock \bibinfo{journal}{Behavioral and brain sciences} \bibinfo{volume}{1}
  (\bibinfo{year}{1978}) \bibinfo{pages}{515--526}.
%Type = Article
\bibitem[{Frith and Frith(2003)}]{frith2003development}
\bibinfo{author}{U.~Frith}, \bibinfo{author}{C.~D. Frith},
\newblock \bibinfo{title}{Development and neurophysiology of mentalizing},
\newblock \bibinfo{journal}{Philosophical Transactions of the Royal Society of
  London. Series B: Biological Sciences} \bibinfo{volume}{358}
  (\bibinfo{year}{2003}) \bibinfo{pages}{459--473}.
%Type = Article
\bibitem[{Vogeley et~al.(2001)Vogeley, Bussfeld, Newen, Herrmann, Happ{\'e},
  Falkai, Maier, Shah, Fink, and Zilles}]{vogeley2001mind}
\bibinfo{author}{K.~Vogeley}, \bibinfo{author}{P.~Bussfeld},
  \bibinfo{author}{A.~Newen}, \bibinfo{author}{S.~Herrmann},
  \bibinfo{author}{F.~Happ{\'e}}, \bibinfo{author}{P.~Falkai},
  \bibinfo{author}{W.~Maier}, \bibinfo{author}{N.~J. Shah},
  \bibinfo{author}{G.~R. Fink}, \bibinfo{author}{K.~Zilles},
\newblock \bibinfo{title}{Mind reading: neural mechanisms of theory of mind and
  self-perspective},
\newblock \bibinfo{journal}{Neuroimage} \bibinfo{volume}{14}
  (\bibinfo{year}{2001}) \bibinfo{pages}{170--181}.
%Type = Book
\bibitem[{Apperly(2010)}]{apperly2010mindreaders}
\bibinfo{author}{I.~Apperly}, \bibinfo{title}{Mindreaders: the cognitive basis
  of" theory of mind"}, \bibinfo{publisher}{Psychology Press},
  \bibinfo{year}{2010}.
%Type = Book
\bibitem[{Johnson-Laird(1983)}]{johnson1983mental}
\bibinfo{author}{P.~N. Johnson-Laird}, \bibinfo{title}{Mental models: Towards a
  cognitive science of language, inference, and consciousness},
  \bibinfo{number}{6}, \bibinfo{publisher}{Harvard University Press},
  \bibinfo{year}{1983}.
%Type = Article
\bibitem[{Gray et~al.(2011)Gray, Jenkins, Heberlein, and
  Wegner}]{gray2011distortions}
\bibinfo{author}{K.~Gray}, \bibinfo{author}{A.~C. Jenkins},
  \bibinfo{author}{A.~S. Heberlein}, \bibinfo{author}{D.~M. Wegner},
\newblock \bibinfo{title}{Distortions of mind perception in psychopathology},
\newblock \bibinfo{journal}{Proceedings of the National Academy of Sciences}
  \bibinfo{volume}{108} (\bibinfo{year}{2011}) \bibinfo{pages}{477--479}.
%Type = Article
\bibitem[{Baron-Cohen et~al.(1999)Baron-Cohen, Ring, Wheelwright, Bullmore,
  Brammer, Simmons, and Williams}]{baron1999social}
\bibinfo{author}{S.~Baron-Cohen}, \bibinfo{author}{H.~A. Ring},
  \bibinfo{author}{S.~Wheelwright}, \bibinfo{author}{E.~T. Bullmore},
  \bibinfo{author}{M.~J. Brammer}, \bibinfo{author}{A.~Simmons},
  \bibinfo{author}{S.~C. Williams},
\newblock \bibinfo{title}{Social intelligence in the normal and autistic brain:
  an fmri study},
\newblock \bibinfo{journal}{European journal of neuroscience}
  \bibinfo{volume}{11} (\bibinfo{year}{1999}) \bibinfo{pages}{1891--1898}.
%Type = Article
\bibitem[{Schaafsma et~al.(2015)Schaafsma, Pfaff, Spunt, and
  Adolphs}]{schaafsma2015deconstructing}
\bibinfo{author}{S.~M. Schaafsma}, \bibinfo{author}{D.~W. Pfaff},
  \bibinfo{author}{R.~P. Spunt}, \bibinfo{author}{R.~Adolphs},
\newblock \bibinfo{title}{Deconstructing and reconstructing theory of mind},
\newblock \bibinfo{journal}{Trends in cognitive sciences} \bibinfo{volume}{19}
  (\bibinfo{year}{2015}) \bibinfo{pages}{65--72}.
%Type = Article
\bibitem[{Apperly and Butterfill(2009)}]{apperly2009humans}
\bibinfo{author}{I.~A. Apperly}, \bibinfo{author}{S.~A. Butterfill},
\newblock \bibinfo{title}{Do humans have two systems to track beliefs and
  belief-like states?},
\newblock \bibinfo{journal}{Psychological review} \bibinfo{volume}{116}
  (\bibinfo{year}{2009}) \bibinfo{pages}{953}.
%Type = Article
\bibitem[{Saxe and Young(2013)}]{saxe2013theory}
\bibinfo{author}{R.~Saxe}, \bibinfo{author}{L.~Young},
\newblock \bibinfo{title}{Theory of mind: How brains think about thoughts},
\newblock \bibinfo{journal}{The Oxford handbook of cognitive neuroscience}
  \bibinfo{volume}{2} (\bibinfo{year}{2013}) \bibinfo{pages}{204--213}.
%Type = Article
\bibitem[{Gopnik and Astington(1988)}]{gopnik1988children}
\bibinfo{author}{A.~Gopnik}, \bibinfo{author}{J.~W. Astington},
\newblock \bibinfo{title}{Children's understanding of representational change
  and its relation to the understanding of false belief and the
  appearance-reality distinction},
\newblock \bibinfo{journal}{Child development}  (\bibinfo{year}{1988})
  \bibinfo{pages}{26--37}.
%Type = Article
\bibitem[{Stahl and Wilson(1995)}]{stahl1995players}
\bibinfo{author}{D.~O. Stahl}, \bibinfo{author}{P.~W. Wilson},
\newblock \bibinfo{title}{On players' models of other players: Theory and
  experimental evidence},
\newblock \bibinfo{journal}{Games and Economic Behavior} \bibinfo{volume}{10}
  (\bibinfo{year}{1995}) \bibinfo{pages}{218--254}.
%Type = Article
\bibitem[{Nagel(1995)}]{nagel1995unraveling}
\bibinfo{author}{R.~Nagel},
\newblock \bibinfo{title}{Unraveling in guessing games: An experimental study},
\newblock \bibinfo{journal}{The American Economic Review} \bibinfo{volume}{85}
  (\bibinfo{year}{1995}) \bibinfo{pages}{1313--1326}.
%Type = Article
\bibitem[{Hedden and Zhang(2002)}]{hedden2002you}
\bibinfo{author}{T.~Hedden}, \bibinfo{author}{J.~Zhang},
\newblock \bibinfo{title}{What do you think i think you think?: Strategic
  reasoning in matrix games},
\newblock \bibinfo{journal}{Cognition} \bibinfo{volume}{85}
  (\bibinfo{year}{2002}) \bibinfo{pages}{1--36}.
%Type = Article
\bibitem[{Keysar et~al.(2003)Keysar, Lin, and Barr}]{keysar2003limits}
\bibinfo{author}{B.~Keysar}, \bibinfo{author}{S.~Lin}, \bibinfo{author}{D.~J.
  Barr},
\newblock \bibinfo{title}{Limits on theory of mind use in adults},
\newblock \bibinfo{journal}{Cognition} \bibinfo{volume}{89}
  (\bibinfo{year}{2003}) \bibinfo{pages}{25--41}.
%Type = Book
\bibitem[{Pinker(2003)}]{pinker2003language}
\bibinfo{author}{S.~Pinker}, \bibinfo{title}{The language instinct: How the
  mind creates language}, \bibinfo{publisher}{Penguin UK},
  \bibinfo{year}{2003}.
%Type = Article
\bibitem[{Leslie and Thaiss(1992)}]{leslie1992domain}
\bibinfo{author}{A.~M. Leslie}, \bibinfo{author}{L.~Thaiss},
\newblock \bibinfo{title}{Domain specificity in conceptual development:
  Neuropsychological evidence from autism},
\newblock \bibinfo{journal}{Cognition} \bibinfo{volume}{43}
  (\bibinfo{year}{1992}) \bibinfo{pages}{225--251}.
%Type = Article
\bibitem[{Saxe(2009)}]{saxe2009theory}
\bibinfo{author}{R.~Saxe},
\newblock \bibinfo{title}{Theory of mind (neural basis)},
\newblock \bibinfo{journal}{Encyclopedia of consciousness} \bibinfo{volume}{2}
  (\bibinfo{year}{2009}) \bibinfo{pages}{401--410}.
%Type = Article
\bibitem[{Heyes(2014)}]{heyes2014submentalizing}
\bibinfo{author}{C.~Heyes},
\newblock \bibinfo{title}{Submentalizing: I am not really reading your mind},
\newblock \bibinfo{journal}{Perspectives on Psychological Science}
  \bibinfo{volume}{9} (\bibinfo{year}{2014}) \bibinfo{pages}{131--143}.
%Type = Article
\bibitem[{Perner and Wimmer(1985)}]{perner1985john}
\bibinfo{author}{J.~Perner}, \bibinfo{author}{H.~Wimmer},
\newblock \bibinfo{title}{“john thinks that mary thinks that…”
  attribution of second-order beliefs by 5-to 10-year-old children},
\newblock \bibinfo{journal}{Journal of experimental child psychology}
  \bibinfo{volume}{39} (\bibinfo{year}{1985}) \bibinfo{pages}{437--471}.
%Type = Article
\bibitem[{Sullivan et~al.(1994)Sullivan, Zaitchik, and
  Tager-Flusberg}]{sullivan1994preschoolers}
\bibinfo{author}{K.~Sullivan}, \bibinfo{author}{D.~Zaitchik},
  \bibinfo{author}{H.~Tager-Flusberg},
\newblock \bibinfo{title}{Preschoolers can attribute second-order beliefs.},
\newblock \bibinfo{journal}{Developmental Psychology} \bibinfo{volume}{30}
  (\bibinfo{year}{1994}) \bibinfo{pages}{395}.
%Type = Article
\bibitem[{Kinderman et~al.(1998)Kinderman, Dunbar, and
  Bentall}]{kinderman1998theory}
\bibinfo{author}{P.~Kinderman}, \bibinfo{author}{R.~Dunbar},
  \bibinfo{author}{R.~P. Bentall},
\newblock \bibinfo{title}{Theory-of-mind deficits and causal attributions},
\newblock \bibinfo{journal}{British Journal of Psychology} \bibinfo{volume}{89}
  (\bibinfo{year}{1998}) \bibinfo{pages}{191--204}.
%Type = Article
\bibitem[{Camerer et~al.(1989)Camerer, Loewenstein, and
  Weber}]{camerer1989curse}
\bibinfo{author}{C.~Camerer}, \bibinfo{author}{G.~Loewenstein},
  \bibinfo{author}{M.~Weber},
\newblock \bibinfo{title}{The curse of knowledge in economic settings: An
  experimental analysis},
\newblock \bibinfo{journal}{Journal of political Economy} \bibinfo{volume}{97}
  (\bibinfo{year}{1989}) \bibinfo{pages}{1232--1254}.
%Type = Article
\bibitem[{Birch and Bloom(2007)}]{birch2007curse}
\bibinfo{author}{S.~A. Birch}, \bibinfo{author}{P.~Bloom},
\newblock \bibinfo{title}{The curse of knowledge in reasoning about false
  beliefs},
\newblock \bibinfo{journal}{Psychological Science} \bibinfo{volume}{18}
  (\bibinfo{year}{2007}) \bibinfo{pages}{382--386}.
%Type = Misc
\bibitem[{Academian(2018)}]{academian2019unrolling}
\bibinfo{author}{Academian}, \bibinfo{title}{Unrolling social metacognition:
  Three levels of meta are not enough.}, \bibinfo{howpublished}{Lesswrong,},
  \bibinfo{year}{2018}.
%Type = Article
\bibitem[{De~Freitas et~al.(2019)De~Freitas, Thomas, DeScioli, and
  Pinker}]{de2019common}
\bibinfo{author}{J.~De~Freitas}, \bibinfo{author}{K.~Thomas},
  \bibinfo{author}{P.~DeScioli}, \bibinfo{author}{S.~Pinker},
\newblock \bibinfo{title}{Common knowledge, coordination, and strategic
  mentalizing in human social life},
\newblock \bibinfo{journal}{Proceedings of the National Academy of Sciences}
  \bibinfo{volume}{116} (\bibinfo{year}{2019}) \bibinfo{pages}{13751--13758}.
%Type = Article
\bibitem[{Goodie et~al.(2012)Goodie, Doshi, and Young}]{goodie2012levels}
\bibinfo{author}{A.~S. Goodie}, \bibinfo{author}{P.~Doshi},
  \bibinfo{author}{D.~L. Young},
\newblock \bibinfo{title}{Levels of theory-of-mind reasoning in competitive
  games},
\newblock \bibinfo{journal}{Journal of Behavioral Decision Making}
  \bibinfo{volume}{25} (\bibinfo{year}{2012}) \bibinfo{pages}{95--108}.
%Type = Book
\bibitem[{Pinker(2007)}]{pinker2007stuff}
\bibinfo{author}{S.~Pinker}, \bibinfo{title}{The stuff of thought: Language as
  a window into human nature}, \bibinfo{publisher}{Penguin},
  \bibinfo{year}{2007}.
%Type = Article
\bibitem[{Pinker et~al.(2008)Pinker, Nowak, and Lee}]{pinker2008logic}
\bibinfo{author}{S.~Pinker}, \bibinfo{author}{M.~A. Nowak},
  \bibinfo{author}{J.~J. Lee},
\newblock \bibinfo{title}{The logic of indirect speech},
\newblock \bibinfo{journal}{Proceedings of the National Academy of sciences}
  \bibinfo{volume}{105} (\bibinfo{year}{2008}) \bibinfo{pages}{833--838}.
%Type = Article
\bibitem[{de~Weerd et~al.(2017)de~Weerd, Verbrugge, and
  Verheij}]{de2017negotiating}
\bibinfo{author}{H.~de~Weerd}, \bibinfo{author}{R.~Verbrugge},
  \bibinfo{author}{B.~Verheij},
\newblock \bibinfo{title}{Negotiating with other minds: the role of recursive
  theory of mind in negotiation with incomplete information},
\newblock \bibinfo{journal}{Autonomous Agents and Multi-Agent Systems}
  \bibinfo{volume}{31} (\bibinfo{year}{2017}) \bibinfo{pages}{250--287}.
%Type = Article
\bibitem[{Verbrugge et~al.(2018)Verbrugge, Meijering, Wierda, van Rijn, Taatgen
  et~al.}]{verbrugge2018stepwise}
\bibinfo{author}{R.~Verbrugge}, \bibinfo{author}{B.~Meijering},
  \bibinfo{author}{S.~Wierda}, \bibinfo{author}{H.~van Rijn},
  \bibinfo{author}{N.~Taatgen}, et~al.,
\newblock \bibinfo{title}{Stepwise training supports strategic second-order
  theory of mind in turn-taking games},
\newblock \bibinfo{journal}{Judgment and decision making} \bibinfo{volume}{13}
  (\bibinfo{year}{2018}) \bibinfo{pages}{79--98}.
%Type = Article
\bibitem[{Curry and Chesters(2012)}]{curry2012putting}
\bibinfo{author}{O.~Curry}, \bibinfo{author}{M.~J. Chesters},
\newblock \bibinfo{title}{‘putting ourselves in the other fellow’s
  shoes’: The role of ‘theory of mind’in solving coordination problems},
\newblock \bibinfo{journal}{Journal of Cognition and Culture}
  \bibinfo{volume}{12} (\bibinfo{year}{2012}) \bibinfo{pages}{147--159}.
%Type = Article
\bibitem[{Devaine et~al.(2014)Devaine, Hollard, and
  Daunizeau}]{devaine2014theory}
\bibinfo{author}{M.~Devaine}, \bibinfo{author}{G.~Hollard},
  \bibinfo{author}{J.~Daunizeau},
\newblock \bibinfo{title}{Theory of mind: did evolution fool us?},
\newblock \bibinfo{journal}{PloS One} \bibinfo{volume}{9}
  (\bibinfo{year}{2014}) \bibinfo{pages}{e87619}.
%Type = Article
\bibitem[{De~Weerd et~al.(2015)De~Weerd, Verbrugge, and Verheij}]{de2015higher}
\bibinfo{author}{H.~De~Weerd}, \bibinfo{author}{R.~Verbrugge},
  \bibinfo{author}{B.~Verheij},
\newblock \bibinfo{title}{Higher-order theory of mind in the tacit
  communication game},
\newblock \bibinfo{journal}{Biologically Inspired Cognitive Architectures}
  \bibinfo{volume}{11} (\bibinfo{year}{2015}) \bibinfo{pages}{10--21}.
%Type = Article
\bibitem[{Erb(2016)}]{erb2016artificial}
\bibinfo{author}{B.~Erb},
\newblock \bibinfo{title}{Artificial intelligence \& theory of mind}
  (\bibinfo{year}{2016}).
%Type = Incollection
\bibitem[{Bolander(2018)}]{bolander2018seeing}
\bibinfo{author}{T.~Bolander},
\newblock \bibinfo{title}{Seeing is believing: Formalising false-belief tasks
  in dynamic epistemic logic},
\newblock in: \bibinfo{booktitle}{Jaakko Hintikka on Knowledge and
  Game-Theoretical Semantics}, \bibinfo{publisher}{Springer},
  \bibinfo{year}{2018}, pp. \bibinfo{pages}{207--236}.
%Type = Article
\bibitem[{Bard et~al.(2020)Bard, Foerster, Chandar, Burch, Lanctot, Song,
  Parisotto, Dumoulin, Moitra, Hughes et~al.}]{bard2020hanabi}
\bibinfo{author}{N.~Bard}, \bibinfo{author}{J.~N. Foerster},
  \bibinfo{author}{S.~Chandar}, \bibinfo{author}{N.~Burch},
  \bibinfo{author}{M.~Lanctot}, \bibinfo{author}{H.~F. Song},
  \bibinfo{author}{E.~Parisotto}, \bibinfo{author}{V.~Dumoulin},
  \bibinfo{author}{S.~Moitra}, \bibinfo{author}{E.~Hughes}, et~al.,
\newblock \bibinfo{title}{The hanabi challenge: A new frontier for ai
  research},
\newblock \bibinfo{journal}{Artificial Intelligence} \bibinfo{volume}{280}
  (\bibinfo{year}{2020}) \bibinfo{pages}{103216}.
%Type = Inproceedings
\bibitem[{Dissing and Bolander(2020)}]{dissing2020implementing}
\bibinfo{author}{L.~Dissing}, \bibinfo{author}{T.~Bolander},
\newblock \bibinfo{title}{Implementing theory of mind on a robot using dynamic
  epistemic logic},
\newblock in: \bibinfo{booktitle}{Proceedings of the {I}nternational {J}oint
  {C}onference on {A}rtificial {I}ntelligence ({IJCAI})}.
%Type = Article
\bibitem[{Thomas et~al.(2014)Thomas, DeScioli, Haque, and
  Pinker}]{thomas2014psychology}
\bibinfo{author}{K.~A. Thomas}, \bibinfo{author}{P.~DeScioli},
  \bibinfo{author}{O.~S. Haque}, \bibinfo{author}{S.~Pinker},
\newblock \bibinfo{title}{The psychology of coordination and common
  knowledge.},
\newblock \bibinfo{journal}{Journal of personality and social psychology}
  \bibinfo{volume}{107} (\bibinfo{year}{2014}) \bibinfo{pages}{657}.
%Type = Article
\bibitem[{Thomas et~al.(2016)Thomas, De~Freitas, DeScioli, and
  Pinker}]{thomas2016recursive}
\bibinfo{author}{K.~A. Thomas}, \bibinfo{author}{J.~De~Freitas},
  \bibinfo{author}{P.~DeScioli}, \bibinfo{author}{S.~Pinker},
\newblock \bibinfo{title}{Recursive mentalizing and common knowledge in the
  bystander effect.},
\newblock \bibinfo{journal}{Journal of Experimental Psychology: General}
  \bibinfo{volume}{145} (\bibinfo{year}{2016}) \bibinfo{pages}{621}.
%Type = Article
\bibitem[{Lee and Pinker(2010)}]{lee2010rationales}
\bibinfo{author}{J.~J. Lee}, \bibinfo{author}{S.~Pinker},
\newblock \bibinfo{title}{Rationales for indirect speech: The theory of the
  strategic speaker.},
\newblock \bibinfo{journal}{Psychological review} \bibinfo{volume}{117}
  (\bibinfo{year}{2010}) \bibinfo{pages}{785}.
%Type = Article
\bibitem[{Thomas et~al.(2018)Thomas, DeScioli, and Pinker}]{thomas2018common}
\bibinfo{author}{K.~A. Thomas}, \bibinfo{author}{P.~DeScioli},
  \bibinfo{author}{S.~Pinker},
\newblock \bibinfo{title}{Common knowledge, coordination, and the logic of
  self-conscious emotions},
\newblock \bibinfo{journal}{Evol Hum Behav} \bibinfo{volume}{39}
  (\bibinfo{year}{2018}) \bibinfo{pages}{179--190}.
%Type = Article
\bibitem[{De~Freitas et~al.(2019)De~Freitas, DeScioli, Thomas, and
  Pinker}]{de2019maimonides}
\bibinfo{author}{J.~De~Freitas}, \bibinfo{author}{P.~DeScioli},
  \bibinfo{author}{K.~A. Thomas}, \bibinfo{author}{S.~Pinker},
\newblock \bibinfo{title}{Maimonides’ ladder: States of mutual knowledge and
  the perception of charitability.},
\newblock \bibinfo{journal}{Journal of Experimental Psychology: General}
  \bibinfo{volume}{148} (\bibinfo{year}{2019}) \bibinfo{pages}{158}.
%Type = Book
\bibitem[{Becker(1976)}]{becker1976economic}
\bibinfo{author}{G.~S. Becker}, \bibinfo{title}{The economic approach to human
  behavior}, \bibinfo{publisher}{University of Chicago press},
  \bibinfo{year}{1976}.
%Type = Article
\bibitem[{Rubinstein(1989)}]{rubinstein1989electronic}
\bibinfo{author}{A.~Rubinstein},
\newblock \bibinfo{title}{The electronic mail game: Strategic behavior under"
  almost common knowledge"},
\newblock \bibinfo{journal}{The American Economic Review}
  (\bibinfo{year}{1989}) \bibinfo{pages}{385--391}.
%Type = Inproceedings
\bibitem[{van Emde~Boas et~al.(1980)van Emde~Boas, Groenendijk, and
  Stokhof}]{van1980conway}
\bibinfo{author}{P.~van Emde~Boas}, \bibinfo{author}{J.~Groenendijk},
  \bibinfo{author}{M.~Stokhof},
\newblock \bibinfo{title}{The conway paradox: Its solution in an epistemic
  framework},
\newblock in: \bibinfo{booktitle}{Proceedings of the third Amsterdam Montague
  Symposion}, pp. \bibinfo{pages}{159--182}.
%Type = Incollection
\bibitem[{van Ditmarsch and Kooi(2015)}]{van2015one}
\bibinfo{author}{H.~van Ditmarsch}, \bibinfo{author}{B.~Kooi},
\newblock \bibinfo{title}{One hundred prisoners and a light bulb},
\newblock in: \bibinfo{booktitle}{One Hundred Prisoners and a Light Bulb},
  \bibinfo{publisher}{Springer}, \bibinfo{year}{2015}, pp.
  \bibinfo{pages}{83--94}.
%Type = Inproceedings
\bibitem[{Meijering et~al.(2010)Meijering, van Maanen, van Rijn, and
  Verbrugge}]{meijering2010facilitative}
\bibinfo{author}{B.~Meijering}, \bibinfo{author}{L.~van Maanen},
  \bibinfo{author}{H.~van Rijn}, \bibinfo{author}{R.~Verbrugge},
\newblock \bibinfo{title}{The facilitative effect of context on second-order
  social reasoning},
\newblock in: \bibinfo{booktitle}{Proceedings of the Annual Meeting of the
  Cognitive Science Society}, volume~\bibinfo{volume}{32}.
%Type = Article
\bibitem[{Wason and Shapiro(1971)}]{wason1971natural}
\bibinfo{author}{P.~C. Wason}, \bibinfo{author}{D.~Shapiro},
\newblock \bibinfo{title}{Natural and contrived experience in a reasoning
  problem},
\newblock \bibinfo{journal}{Quarterly Journal of Experimental Psychology}
  \bibinfo{volume}{23} (\bibinfo{year}{1971}) \bibinfo{pages}{63--71}.
%Type = Article
\bibitem[{Verbrugge and Mol(2008)}]{verbrugge2008learning}
\bibinfo{author}{R.~Verbrugge}, \bibinfo{author}{L.~Mol},
\newblock \bibinfo{title}{Learning to apply theory of mind},
\newblock \bibinfo{journal}{Journal of Logic, Language and Information}
  \bibinfo{volume}{17} (\bibinfo{year}{2008}) \bibinfo{pages}{489--511}.
%Type = Article
\bibitem[{Chen et~al.(2016)Chen, Schonger, and Wickens}]{ChenSchongerWickens16}
\bibinfo{author}{D.~Chen}, \bibinfo{author}{M.~Schonger},
  \bibinfo{author}{C.~Wickens},
\newblock \bibinfo{title}{otree—an open-source platform for laboratory,
  online, and field experiments},
\newblock \bibinfo{journal}{Journal of Behavioral and Experimental Finance}
  \bibinfo{volume}{9} (\bibinfo{year}{2016}) \bibinfo{pages}{88--97}.
%Type = Article
\bibitem[{Seidenfeld(1985)}]{seidenfeld1985calibration}
\bibinfo{author}{T.~Seidenfeld},
\newblock \bibinfo{title}{Calibration, coherence, and scoring rules},
\newblock \bibinfo{journal}{Philosophy of Science} \bibinfo{volume}{52}
  (\bibinfo{year}{1985}) \bibinfo{pages}{274--294}.
%Type = Article
\bibitem[{Palfrey and Wang(2009)}]{palfrey2009eliciting}
\bibinfo{author}{T.~R. Palfrey}, \bibinfo{author}{S.~W. Wang},
\newblock \bibinfo{title}{On eliciting beliefs in strategic games},
\newblock \bibinfo{journal}{Journal of Economic Behavior \& Organization}
  \bibinfo{volume}{71} (\bibinfo{year}{2009}) \bibinfo{pages}{98--109}.
%Type = Book
\bibitem[{Shoham and Leyton-Brown(2008)}]{shoham2008multiagent}
\bibinfo{author}{Y.~Shoham}, \bibinfo{author}{K.~Leyton-Brown},
  \bibinfo{title}{Multiagent systems: Algorithmic, game-theoretic, and logical
  foundations}, \bibinfo{publisher}{Cambridge University Press},
  \bibinfo{year}{2008}.
\end{thebibliography}
\normalsize
\fi

\end{document}