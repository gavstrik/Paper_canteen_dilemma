plot(0,0, type='n', xlim=c(0,1), ylim=yLim, ann=FALSE)
polygon(c(x,rev(x)), c(lo, rev(hi)), border=NA, col=add.alpha('black',.25))
abline(h=0, v=0.5, lty=3, col=add.alpha('black',.75))
lines(x,y)
# x1 = c(0,1,1,0)
# y1 = c(rep(lm.coef[i,2],2),rep(lm.coef[i,3],2))
# polygon(x1, y1, col=add.alpha('red',.15), border=NA)
# abline(h=lm.coef[i,1], col=add.alpha('red',.75))
# mtext(rownames(coef(fit.qr))[i]  , 3, line=0)
if(i==1){
mtext(text = "Intercept", side=2, line=2)
mtext(text = "v=0", side=3, line=1)
# legend("topleft", c("Quantile","Linear"), col=add.alpha(c("black","red"),.75), lty=1, lwd=2, bty='n')
} else if(i==2){
mtext(text = "Slope", side=2, line=2)
} else if(i==3){
mtext(text = "v=1", side=3, line=1)
} else if(i==4){
mtext(text = "v=3", side=3, line=1)
} else if(i==5){
mtext(text = "v=9", side=3, line=1)
}
}
mtext("Reference sequence", side=3, line = 1.7, outer=TRUE, adj = .06)
mtext("Contrast sequences", side=3, line = 1.7, outer=TRUE, adj = .65)
mtext("Quantiles", side=1, line = 1, outer=TRUE)
#mtext(method_type, side=3, line = 4, outer=TRUE, cex=1.5)
if(savePDF) dev.off()
tmp = aggregate(logerr~d+v+method, data=dots, subset=(method=='history'), median)[1:4,]
summary(lm(logerr~I(log(d)-log(55)), data=tmp))
# par(mfrow=c(6,7), mar=c(0,0,0,0), oma=c(0,0,0,0))
# S = unique(dots$session)
# for(s in S){
#   tmp = subset(dots, session==s)
#   acf(tmp$logerr)
# }
# Plot regression lines for quantiles comparing within v-groups:
taus = seq(0.1,.9,.2)
ntau = length(taus)
if(method_type == "Pristine"){
# History group
fit.lm = lm(logerr ~ logd2*as.factor(v), data=dots, subset=(method=="history"))
fit.qr = rq(logerr ~ logd2*as.factor(v), data=dots, subset=(method=="history"), tau=taus)
} else{
# Manipulated group
fit.lm = lm(logerr ~ logd2*as.factor(v), data=dots, subset=(method!="history" | v==0))
fit.qr = rq(logerr ~ logd2*as.factor(v), data=dots, subset=(method!="history" | v==0), tau=taus)
}
coef(fit.qr)
v0 = coef(fit.qr)[1:2,]
v1 = coef(fit.qr)[1:2,]+coef(fit.qr)[c(3,6),]
v3 = coef(fit.qr)[1:2,]+coef(fit.qr)[c(4,7),]
v9 = coef(fit.qr)[1:2,]+coef(fit.qr)[c(5,8),]
plot_qrlines <- function(v,x, yLim = c(0,4), logscale=FALSE){
taus = as.numeric(substr(colnames(v0),6, nchar(colnames(v0))))
ntau = ncol(v0)
plot(0,0, xlim=range(exp(x+log(55))), ylim=yLim, type='n', bty='n')
for(i in 1:ntau){
if(taus[i]==.5){
cols = add.alpha('tomato2',.95)
Lty  = 1
Lwd  = 1.5
} else{
cols = add.alpha('tomato2',.95)
Lty  = 2
Lwd  = 1.5
}
if(logscale){
abline(h=0, lty=3, col=add.alpha('black',.5))
lines(exp(x+log(55)), exp(v[1,i]+v[2,i]*x), col=cols, lty=Lty, lwd=Lwd)
text(max(exp(x+log(55))),exp(v[1,i]+v[2,i]*max(x)), labels = taus[i], pos = 4, cex=.75, offset = .1)
} else{
abline(h=1, lty=3, col=add.alpha('black',.5))
lines(exp(x+log(55)), exp(v[1,i]+v[2,i]*x), col=cols, lty=Lty, lwd=Lwd)
text(max(exp(x+log(55))),exp(v[1,i]+v[2,i]*max(x)), labels = taus[i], pos = 4, cex=.75, offset = .1)
}
}
}
fn = paste0(plotlib,tolower(method_type),"_qrlines.pdf")
# Plot results
if(savePDF) pdf(file=fn, width=10, height=6)
par(mfrow=c(2,2), mar=c(3,3,1,1), oma=c(1.5,1.5,3.5,0))
x = seq(0,3.1,.1)
plot_qrlines(v0,x); mtext("v=0", side=3, line=0)
tmp = subset(dots, method=="history" & v==0)
points(tmp$d,exp(tmp$logerr), pch=16, col=add.alpha('black',.25), cex=.5)
plot_qrlines(v1,x); mtext("v=1", side=3, line=0)
tmp = subset(dots, method=="history" & v==1)
points(tmp$d,exp(tmp$logerr), pch=16, col=add.alpha('black',.25), cex=.5)
plot_qrlines(v3,x); mtext("v=3", side=3, line=0)
tmp = subset(dots, method=="history" & v==3)
points(tmp$d,exp(tmp$logerr), pch=16, col=add.alpha('black',.25), cex=.5)
plot_qrlines(v9,x); mtext("v=9", side=3, line=0)
tmp = subset(dots, method=="history" & v==9)
points(tmp$d,exp(tmp$logerr), pch=16, col=add.alpha('black',.25), cex=.5)
mtext("Number of dots visible", side=1, outer=TRUE, line=0)
mtext("Relative error", side=2, outer=TRUE, line=0)
mtext(method_type, side=3, line = 2, outer=TRUE, cex=1.5)
if(savePDF) dev.off()
# Plot regression lines for quantiles comparing within quantile-groups
# vectors selecting parameters:
#   a0 = as.logical(c(1,0,0,0,0,0,0,0))
#   a1 = as.logical(c(1,0,1,0,0,0,0,0))
#   a3 = as.logical(c(1,0,0,1,0,0,0,0))
#   a9 = as.logical(c(1,0,0,0,1,0,0,0))
#   b0 = as.logical(c(0,1,0,0,0,0,0,0))
#   b1 = as.logical(c(0,1,0,0,0,1,0,0))
#   b3 = as.logical(c(0,1,0,0,0,0,1,0))
#   b9 = as.logical(c(0,1,0,0,0,0,0,1))
#   A = t(as.matrix(data.frame(a0=a0, a1=a1, a3=a3, a9=a9, b0=b0,b1=b1,b3=b3,b9=b9)))
#
#
# get_qrline = function(x,a,b, est, C){
#   nx = length(x)
#   A  = outer(rep(1,nx),a,"*")+outer(x,b,"*")
#   se = sqrt(diag(A%*%C%*%t(A)))
#   y  = A%*%est
#   hi = y+1.96*se
#   lo = y-1.96*se
#   out = data.frame(y=y, lo=lo, hi=hi)
#   return(out)
# }
#
# taus = c(.1,.25,.5,.75,.9)
# ntau = length(taus)
# M = aggregate(logerr~d+v+method, data=dots, function(x) quantile(x, taus))
# fit.qr = rq(logerr ~ logd2*as.factor(v), data=dots, subset=(method=="history"), tau=taus)
# summary(fit.qr)
# sum.qr = summary(fit.qr, covariance=TRUE)
#
# plot_qrtau <- function(fit){
#
#   # fit = 2
#   tmp = sum.qr[[fit]]
#   C   = tmp$cov
#   est = tmp$coefficients[,1]
#   x   = seq(0,3,.01)
#
#   qrl0 = get_qrline(x,a0,b0, est, C)
#   qrl1 = get_qrline(x,a1,b1, est, C)
#   qrl3 = get_qrline(x,a3,b3, est, C)
#   qrl9 = get_qrline(x,a9,b9, est, C)
#
#   xx = c(x,rev(x))
#   y0 = c(qrl0$lo, rev(qrl0$hi))
#   y1 = c(qrl1$lo, rev(qrl1$hi))
#   y3 = c(qrl3$lo, rev(qrl3$hi))
#   y9 = c(qrl9$lo, rev(qrl9$hi))
#   col0 = 'tomato2'
#   col1 = 'steelblue'
#   col3 = 'limegreen'
#   col9 = 'magenta'
#   plot(0,0, xlim=c(0,1200), ylim=c(0,2), type='n')
#   polygon(55*exp(xx), exp(y0), border=NA, col=add.alpha(col0,.3))
#   polygon(55*exp(xx), exp(y1), border=NA, col=add.alpha(col1,.3))
#   polygon(55*exp(xx), exp(y3), border=NA, col=add.alpha(col3,.3))
#   polygon(55*exp(xx), exp(y9), border=NA, col=add.alpha(col9,.3))
#   abline(h=1, lty=3, col=add.alpha('black',.75))
#
#   tmp = subset(M, method=="history" & v==0)
#   points(tmp$d, exp(tmp$logerr[,fit]), pch=16, col=add.alpha(col0,.75))
#   tmp = subset(M, method=="history" & v==1)
#   points(tmp$d, exp(tmp$logerr[,fit]), pch=16, col=add.alpha(col1,.75))
#   tmp = subset(M, method=="history" & v==3)
#   points(tmp$d, exp(tmp$logerr[,fit]), pch=16, col=add.alpha(col3,.75))
#   tmp = subset(M, method=="history" & v==9)
#   points(tmp$d, exp(tmp$logerr[,fit]), pch=16, col=add.alpha(col9,.75))
#
#   # tmp = subset(dots, method=="history")
#   # points(tmp$d, exp(tmp$logerr), pch=16, col=add.alpha('black',.15))
#
# }
# taus
# ntau
# par(mfrow=c(2,3))
# for(i in 1:ntau){
#   plot_qrtau(i)
# }
#
View(v0)
View(v1)
View(lm.coef)
View(fit.qr)
View(sum.lm)
View(get_coef)
View(v0)
View(v1)
View(v0)
View(v0)
View(lm.coef)
View(lm.coef)
View(lm.coef)
View(fit.lm)
View(v0)
View(v1)
View(v0)
View(v0)
#########################################################################################################################
#   Aux script for the quantile regression analysis of dots.rda used in the paper:
#     "The Effects of Bounded Social Information on Sequential Decision Making" - Engelhardt et al (2021)
#
#   Author:   Jacob Stærk-Østergaard,
#   email:    ostergaard@sund.ku.dk
#
#########################################################################################################################
library(WoT)
# misc::clean_up() # cleaning the workspace
load("data/qr_est.rda")
ndots = c(55,148,403,1097)
#########################################################################################################################
# Control group estimates
#########################################################################################################################
# qr$sum$qr$historical
rownames(qr$fit$qr$historical$coefficients)
tmp = qr$est$qr$historical[-1,6,]
colnames(tmp) = rownames(qr$fit$qr$historical$coefficients)
v0 = tmp[,1:2]
B   = outer(v0[,2],log(ndots/55),"*")
v0 = v0[,1]+B
colnames(v0) = paste0("d=",ndots)
round(v0,4)
round(exp(v0)*matrix(ndots,nr=3,nc=4,byrow=TRUE))
round(exp(v0),4)
v0 = list(log_ratio = v0, ratio = exp(v0), dots = round(exp(v0)*matrix(ndots,nr=3,nc=4,byrow=TRUE)))
v0 = lapply(v0,t)
#########################################################################################################################
# v=1,3,9 groups: (refit model at tau=.5 with v=1,3,9 as reference groups)
#########################################################################################################################
dots$logerr = log(dots$guess/dots$d)  # create logerr variable to be analyzed
dots$logd2  = log(dots$d)-log(55)     # adjust the number dots so that the d=55 group intercept parameter corresponds to the group quantile
dots$v      = as.factor(dots$v)       # interpret the number of previous estimates visible as a factor variable
dots$v1     = relevel(dots$v,ref="1")
dots$v3     = relevel(dots$v,ref="3")
dots$v9     = relevel(dots$v,ref="9")
get_coef <- function(x, a=.05){
est = x$coefficients[,1]
hi  = x$coefficients[,1]+qnorm(1-a/2)*x$coefficients[,2]
lo  = x$coefficients[,1]-qnorm(1-a/2)*x$coefficients[,2]
out = data.frame(tau=x$tau, est = est, lo=lo, hi=hi)
return(out)
}
# Set quantiles for regression
tau = .5
qr1 = list(historical  = rq(logerr ~ logd2*v1, data=dots, subset=(method=="history"), tau=tau),
manipulated = rq(logerr ~ logd2*v1, data=dots, subset=(method!="history" | v==0), tau=tau))
qr3 = list(historical  = rq(logerr ~ logd2*v3, data=dots, subset=(method=="history"), tau=tau),
manipulated = rq(logerr ~ logd2*v3, data=dots, subset=(method!="history" | v==0), tau=tau))
qr9 = list(historical  = rq(logerr ~ logd2*v9, data=dots, subset=(method=="history"), tau=tau),
manipulated = rq(logerr ~ logd2*v9, data=dots, subset=(method!="history" | v==0), tau=tau))
make_tables <- function(model){
tmp1 = get_coef(summary(model, covariance=TRUE))
tmp2 = t(tmp1[1:2,-1])
tmp3 = outer(tmp2[,2],log(ndots/55),"*")
tmp2  = tmp2[,1]+tmp3
colnames(tmp2) = paste0("d=",ndots)
out = list(log_ratio = tmp2, ratio = exp(tmp2), dots = round(exp(tmp2)*matrix(ndots,nr=3,nc=4,byrow=TRUE)))
out = lapply(out,t)
return(out)
}
## Naming: vxy, where x refers to dots and y to historical or manipulated
res = list(v1h = make_tables(qr1$historical),
v1m = make_tables(qr1$manipulated),
v3h = make_tables(qr3$historical),
v3m = make_tables(qr3$manipulated),
v9h = make_tables(qr9$historical),
v9m = make_tables(qr9$manipulated))
res = lapply(res, function(x)  matrix(unlist(x), nrow=4))
xtable::xtable(matrix(unlist(v0),nrow=4), digits=4)
lapply(res, xtable::xtable)
load("~/Papers/Paper_dot_experiments/Data/qr_est.rda")
setwd("C:/Users/hjl161/Documents/Papers/Paper_canteen_dilemma/R/mixed effects")
require(ggplot2)
require(GGally)
require(reshape2)
require(lme4)
require(compiler)
require(parallel)
require(boot)
require(lattice)
require(cowplot)
cd <- read.csv("../../Data/data.csv")
head(cd)
cd$arr <- as.numeric(as.factor(cd$arrival))
tail(cd)
keeps <- c("exp", "pair_id", "code", "round", "arr", "choice", "certainty")
Data <- cd[keeps]
head(Data)
keeps <- c("exp", "pair_id", "code", "round", "arr", "choice", "certainty")
Data <- cd[keeps]
# make a new column giving each pair of subjects an unique id:
cd$pair_id <- paste(cd$session, cd$group)
head(cd)
# change arrival times to consecutive numbers
cd$arr <- as.numeric(as.factor(cd$arrival))
tail(cd)
keeps <- c("exp", "pair_id", "code", "round", "arr", "choice", "certainty")
Data <- cd[keeps]
head(Data)
Data <- within(Data, {
choice <- factor(choice, levels = 0:1, labels = c("office", "canteen"))
arr <- factor(arr)
exp <- factor(exp)
certainty <- factor(certainty)
round <- factor(round)
})
str(Data)
log2 <- glm(choice ~ arr * experiment, data=cd, family="binomial")
log2 <- glm(choice ~ arr * exp, data=cd, family="binomial")
summary(log2)
log2 <- glm(choice ~ arr + exp, data=cd, family="binomial")
summary(log2)
ll.null <- log2$null.deviance/-2
ll.proposed <- log2$null.deviance/-2
(ll.null - ll.proposed) / ll.null
predicted.data <- data.frame(probability.of.choosing.canteen=log2$fitted.values, choice=cd$choice)
predicted.data <- predicted.data[order(predicted.data$probability.of.choosing.canteen, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(prediced.data$arr)
predicted.data$rank <- 1:nrow(predicted.data$arr)
predicted.data$rank <- predicted.data$arr
ggplot(data=predicted.data, aes(x=rank, y=probability.of.choosing.canteen)) +
geom_point(aes(color=cd), alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of choosing canteen")
log2 <- glm(choice ~ arr + exp, data=Data, family="binomial")
summary(log2)
ll.null <- log2$null.deviance/-2
ll.proposed <- log2$null.deviance/-2
(ll.null - ll.proposed) / ll.null
predicted.data <- data.frame(probability.of.choosing.canteen=log2$fitted.values, choice=Data$choice)
predicted.data <- predicted.data[order(predicted.data$probability.of.choosing.canteen, decreasing=FALSE),]
predicted.data$rank <- predicted.data[order(predicted.data$arr, decreasing=FALSE),]
predicted.data$rank <- order(predicted.data$arr, decreasing=FALSE)
predicted.data$rank <- c(1,2,3,4,5,6,7,8)
ggplot(data=predicted.data, aes(x=rank, y=probability.of.choosing.canteen)) +
geom_point(aes(color=cd), alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of choosing canteen")
predicted.data <- data.frame(probability.of.choosing.canteen=log2$fitted.values, arr=Data$arr, choice=Data$choice)
predicted.data <- predicted.data[order(predicted.data$probability.of.choosing.canteen, decreasing=FALSE),]
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
geom_point(aes(color=Data), alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of choosing canteen")
ll.null <- log2$null.deviance/-2
ll.proposed <- log2$deviance/-2
(ll.null - ll.proposed) / ll.null
1 - pchisq(2*(ll.proposed - ll.null), df=1)
1 - pchisq((logistic$null.deviance - logistic$deviance), df=1)
## chi-square value = 2*(LL(Proposed) - LL(Null))
## p-value = 1 - pchisq(chi-square value, df = 2-1)
1 - pchisq(2*(ll.proposed - ll.null), df=1)
1 - pchisq((log2$null.deviance - log2$deviance), df=1)
## The p-value for the R^2
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))
## The p-value for the R^2
1 - pchisq(2*(ll.proposed - ll.null), df=(length(log2$coefficients)-1))
log2 <- glm(choice ~ arr, data=Data, family="binomial")
summary(log2)
## NOTE: Since we are doing logistic regression...
## Null devaince = 2*(0 - LogLikelihood(null model))
##               = -2*LogLikihood(null model)
## Residual deviacne = 2*(0 - LogLikelihood(proposed model))
##                   = -2*LogLikelihood(proposed model)
ll.null <- log2$null.deviance/-2
ll.proposed <- log2$deviance/-2
## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null)
(ll.null - ll.proposed) / ll.null
## chi-square value = 2*(LL(Proposed) - LL(Null))
## p-value = 1 - pchisq(chi-square value, df = 2-1)
1 - pchisq(2*(ll.proposed - ll.null), df=1)
1 - pchisq((log2$null.deviance - log2$deviance), df=1)
## The p-value for the R^2
1 - pchisq(2*(ll.proposed - ll.null), df=(length(log2$coefficients)-1))
predicted.data <- data.frame(probability.of.choosing.canteen=log2$fitted.values, arr=Data$arr, choice=Data$choice)
predicted.data <- predicted.data[order(predicted.data$probability.of.choosing.canteen, decreasing=FALSE),]
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
geom_point(aes(color=Data), alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
log2 <- glm(choice ~ arr*exp, data=Data, family="binomial")
summary(log2)
## NOTE: Since we are doing logistic regression...
## Null devaince = 2*(0 - LogLikelihood(null model))
##               = -2*LogLikihood(null model)
## Residual deviacne = 2*(0 - LogLikelihood(proposed model))
##                   = -2*LogLikelihood(proposed model)
ll.null <- log2$null.deviance/-2
ll.proposed <- log2$deviance/-2
## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null)
(ll.null - ll.proposed) / ll.null
## chi-square value = 2*(LL(Proposed) - LL(Null))
## p-value = 1 - pchisq(chi-square value, df = 2-1)
1 - pchisq(2*(ll.proposed - ll.null), df=1)
1 - pchisq((log2$null.deviance - log2$deviance), df=1)
## The p-value for the R^2
1 - pchisq(2*(ll.proposed - ll.null), df=(length(log2$coefficients)-1))
predicted.data <- data.frame(probability.of.choosing.canteen=log2$fitted.values, arr=Data$arr, choice=Data$choice)
predicted.data <- predicted.data[order(predicted.data$probability.of.choosing.canteen, decreasing=FALSE),]
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
geom_point(aes(color=exp),alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
geom_smooth(method="lm") +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
aes(AMT, DTU1, DTU2=exp, colour=exp, fill=exp) +
geom_smooth(method="lm") +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
#aes(AMT, DTU1, DTU2=exp, colour=exp, fill=exp) +
geom_smooth(method="glm") +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, color = exp)) +
#aes(AMT, DTU1, DTU2=exp, colour=exp, fill=exp) +
geom_smooth(method="glm") +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, color = exp)) +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
predicted.data <- data.frame(probability.of.choosing.canteen=log2$fitted.values, arr=Data$arr, choice=Data$choice, exp=Data$exp)
predicted.data <- predicted.data[order(predicted.data$probability.of.choosing.canteen, decreasing=FALSE),]
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen)) +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point(alpha=1, shape=4, stroke=2) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point() +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point() +
geom_smooth(method = "lm", fill = NA)
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point() +
geom_smooth(method = "glm", fill = NA)
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point() +
geom_smooth(method = "glm", family="binomial", colour=exp, size=1.5)
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point() +
binomial_smooth()
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point() +
geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point() +
geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)+
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
#geom_point() +
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point() +
geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point() +
geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
ggplot(data=predicted.data, aes(x=arr, y=probability.of.choosing.canteen, colour = exp)) +
geom_point() +
#geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
xlab("Arrival") +
ylab("Predicted probability of choosing canteen")
m <- glmer(choice ~ exp*arr + (1 | pair_id), data = Data, family = binomial,
control = glmerControl(optimizer = "bobyqa"), nAGQ = 10)
summary(m)
ll.null <- m$null.deviance/-2
mpredicted.data <- data.frame(probability.of.choosing.canteen=m$fitted.values, arr=Data$arr, choice=Data$choice, exp=Data$exp)
m <- glmer(choice ~ exp*arr + (1 | code), data = Data, family = binomial,
control = glmerControl(optimizer = "bobyqa"), nAGQ = 10)
summary(m)
